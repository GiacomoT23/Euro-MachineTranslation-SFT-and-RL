{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiacomoT23/Euro-MachineTranslation-SFT-and-RL/blob/main/de_en_SFT_gh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9FMvIaq_yVO"
      },
      "source": [
        "# Model download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeZ8YIdNjicj"
      },
      "source": [
        "As model to be finetuned, Qwen2.5 1.5B base was selected. It was chosen because of its size compatible with the available resources and because it was pretrained also on the selected languages and much more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVzWFAYRkDhz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-input --upgrade pip\n",
        "!pip install --no-input unsloth bitsandbytes accelerate peft \"trl>=0.9.0\" sentencepiece protobuf hf_transfer\n",
        "!pip install --no-input \"transformers==4.55.4\"\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch, os\n",
        "\n",
        "USE_4BIT = True\n",
        "USE_GC   = \"unsloth\"\n",
        "MAX_SEQ  = 256\n",
        "MODEL_ID = \"unsloth/Qwen2.5-1.5B\"\n",
        "\n",
        "dtype = None if USE_4BIT else torch.float16\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = MODEL_ID,\n",
        "    max_seq_length = MAX_SEQ,\n",
        "    dtype          = dtype,\n",
        "    load_in_4bit   = USE_4BIT,\n",
        ")\n",
        "\n",
        "# LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8,\n",
        "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_alpha = 8,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = USE_GC,\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model: {MODEL_ID} | 4bit={USE_4BIT} | GC={USE_GC} | seq={MAX_SEQ}\")\n",
        "print(f\"Trainable params (LoRA+heads): ~{n_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cajPQvq3ZEqZ"
      },
      "source": [
        "# Loading splits\n",
        "Loading the preprocessed dataset saved on drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMHJSmv0ZHDA",
        "outputId": "a52131e9-aa42-4262-b111-94ab80f0ae81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "{'train': 62240, 'validation': 3000, 'test': 3003}\n"
          ]
        }
      ],
      "source": [
        "# === Load dataset filtrato da Google Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from datasets import load_from_disk, DatasetDict\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/mt_datasets/wmt14_de_en_proc_100k_len203_sim082_lid099\"\n",
        "ds_proc = load_from_disk(SAVE_DIR)\n",
        "\n",
        "train_filt = ds_proc.get(\"train\")\n",
        "val_filt   = ds_proc.get(\"validation\")\n",
        "test_filt  = ds_proc.get(\"test\")\n",
        "\n",
        "print({k: len(v) for k, v in ds_proc.items()})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vBsNk5FaKc"
      },
      "source": [
        "# Formatting for SFT in unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "05e555160dd74634992d90166a616d78",
            "9201b8b43d034b7fab43c547ee223618",
            "69e9016ba4e8470cb254acca791f0de6",
            "77f4f50719fc4f48ac021a2e1da03c0f",
            "75244933d14a4b7d8ea5e3c50148435d",
            "3026b3da4b6b406a984113b35bdaa47d",
            "60c55842c86c4c9cbbbb8a1821ddaca0",
            "a34214b815c643059d2c3bb23b7dcb33",
            "a4b2bf9b6f884b0ca3690f8e1f3077cc",
            "97f0b8fa8c174e50a4a3d8a4c1ae429d",
            "5f92278d01524ca8a7a749bc17d58ee9",
            "9fb51ba05dc449f4a8008062e288cc40",
            "47d5822edcc945c4b8912ce5ee9c4593",
            "4c1dc9a3218b44cba18038c73c2eaf43",
            "ac96617f95e64896a2bfd1bf4a85737f",
            "f93cdf3b965545909fdf9178e5e0fef1",
            "f436a18b442e42e199bb9e346126072d",
            "3643d01002a54ffe87a93f01e516c444",
            "bbd1f6c3263c4f47bef86e2f320a7470",
            "e6cc5da5a73c421ba2ff19abd72a680c",
            "673e2cb83ba54bb5b92c6f89d12590af",
            "443aa4c6cd60497781e468a61442af51",
            "b5e4e4ff1f4244b4a120de42a3c44aee",
            "432313f25441452ab2691dd28ebe94ff",
            "8b2a8b6af4d7410da0ebe061b48f0609",
            "185c2700e3cd4570a009fd5d1b3c61a5",
            "3082780cf71947d3b55e872f58c8818a",
            "e3a445dc1f534e639a9ce4a0b6c7e3a3",
            "153d418cdbe34db1abe5c17556d4f30e",
            "a1f9db52571d44b289485b70ab998ae7",
            "ab018df0b40a4f3a9bd948f698839664",
            "48268e7db46743eeaadc776b109b10c0",
            "ae24121d5165469ba6446ca98aa1d1ed",
            "7a4bd01db9184e3bb1dc9d1302a7f4bb",
            "d58759b73d714a42a9ade016b6a5f2ae",
            "fb91994db2d746ec96c8c9a2820c8715",
            "5b0d0d58f3b647f89bd336cb5c96d54e",
            "af3f728d77724f8e9d4de95fb434ca8b",
            "df70001bfc66490fb1c18fe640565481",
            "542d9b6593894b01bbb2d46579ff8263",
            "cf74b9b0fa034be0a4d4ead60745c139",
            "5842fa51732c4a99b4760d77f5e72cf6",
            "630d4bc417fd4d56b5e7313d62ad2c95",
            "a0e277b2cfde4ab290391d60512da795",
            "aaa12067b3d64b068d2169f32268002e",
            "ccf76387350a4acebbfbfe01382c46af",
            "23905d02a1bb41bba225a6caa2b39a46",
            "da19e8f28f214286a9c429cce07d27bf",
            "1444c52aeff9424799ada46fd4fdfee4",
            "8d604ebc33df41059381a29e139e84d0",
            "352eebecf4ec4d3881ba599f4cc9d1f2",
            "2ec54286f0f642dbad4be933ff2636a0",
            "2b45d20ea05a434bb1fb275908f013f1",
            "a22de01cd28e4701a779c24684144455",
            "2ef46aabd1394359ba28f3832971e5d1",
            "b259df3ffdf24588acdae2eb6a7df82e",
            "90a18eed7ab94ef689de8b624fb2c564",
            "7ec67779944b4a56a49e9af48e38786e",
            "1336a6fe19cb4f26b8f4078f348fc9d8",
            "c3d6345907184f728754bfb78aa29cca",
            "6ad766f791ea4253995b114dcbc0f142",
            "45d3ab97514343cebabb5d418a380f33",
            "a348b06b2cef4ff7a31cde4ecfaf0a39",
            "8b09e120de24476e85d2e7d8b9ca216c",
            "365512f7005f445ca30f5bf80e0a4628",
            "1977e134df15425abc332336c46f6b63",
            "af81a6e734564a45817d34e111e44b51",
            "29d7f2b24b3146b696f9b9965df8b1bc",
            "f67d12e715d14d3ab29b3d0cdbb755fb",
            "d739f1b6c944422897fdd4285fde3d93",
            "21e7f4b3872b4f05ac98e122d2326b19",
            "9a8e6ef9930140e8bc69310dc6023129",
            "d975eda5fb4b4a02b6a35ad39b197a40",
            "ed98277f10cc44a1a13473916c2404d2",
            "d3ac3ac9970c4c758ae9e3ef4300282a",
            "929fa700c71543d3aa8a907a05952d01",
            "6b0f4a731a9b458cb85226c3b590489d",
            "aae47c2f589e4d1c9095c0670008e635",
            "70424bf99d98442bad356980055cf2ce",
            "1a65511b533247d38443c10b54076fc4",
            "d2b02dcd468a40579b1a66da0bb796dc",
            "1b037ca8f697431d84eb598f92279f57",
            "6dc81de6c89445e68e70a86db62f9eaf",
            "81af462c4f264d4c94d55a500062082d",
            "bb3ac4507b6b454cbafb21ceed183a48",
            "65e5fa012d1c47d6bf12cafb277559ca",
            "e8276d811c654d92a2b83e96e4be97b9",
            "8bfb895afce64260ba333bd1ca98fc7d"
          ]
        },
        "id": "HOW7mPcIl6We",
        "outputId": "5585c93d-e150-41ce-dd1a-eb509a01b88b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[normalize] add src_txt/tgt_txt from translation:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[normalize] add src_txt/tgt_txt from translation:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Adding language columns:   0%|          | 0/62240 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Adding language columns:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[format] train:   0%|          | 0/62240 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/62240 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[format] val:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val sizes: 62240 3000\n"
          ]
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "import unicodedata\n",
        "\n",
        "SRC, TGT = \"de\", \"en\"\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize(\"NFKC\", (s or \"\"))\n",
        "\n",
        "def ensure_src_tgt_cols(dset):\n",
        "    \"\"\"Return dataset with columns src_txt/tgt_txt.\n",
        "    \"\"\"\n",
        "    cols = set(dset.column_names)\n",
        "    if {\"src_txt\",\"tgt_txt\"}.issubset(cols):\n",
        "        return dset\n",
        "    elif \"translation\" in cols:\n",
        "        def to_cols(batch):\n",
        "            src = [nfkc(x.get(SRC,\"\")) for x in batch[\"translation\"]]\n",
        "            tgt = [nfkc(x.get(TGT,\"\")) for x in batch[\"translation\"]]\n",
        "            return {\"src_txt\": src, \"tgt_txt\": tgt}\n",
        "        return dset.map(\n",
        "            to_cols,\n",
        "            batched=True,\n",
        "            batch_size=4096,\n",
        "            desc=\"[normalize] add src_txt/tgt_txt from translation\",\n",
        "        )\n",
        "    else:\n",
        "        raise KeyError(\"Lo split non ha né src_txt/tgt_txt né translation.\")\n",
        "\n",
        "train_norm = ensure_src_tgt_cols(train_filt)\n",
        "val_norm   = ensure_src_tgt_cols(val_filt)\n",
        "test_norm = ensure_src_tgt_cols(test_filt)\n",
        "\n",
        "LANG_NAME = {\"de\": \"German\", \"en\": \"English\"}\n",
        "def lang_name(code: str) -> str:\n",
        "    return LANG_NAME.get(code.lower(), code.upper())\n",
        "\n",
        "MT_PROMPT = \"\"\"Translate the following sentence from {src_name} to {tgt_name}.\n",
        "Source ({src_code}):\n",
        "{src}\n",
        "Translation:\n",
        "{tgt}\"\"\"\n",
        "\n",
        "GEN_PROMPT = \"\"\"Translate the following sentence from {src_name} to {tgt_name}.\n",
        "Source ({src_code}):\n",
        "{src}\n",
        "Translation:\n",
        "\"\"\"\n",
        "\n",
        "EOS = tokenizer.eos_token or \"\"\n",
        "\n",
        "def add_lang_cols(dset, src_code=SRC, tgt_code=TGT):\n",
        "    if dset is None: return None\n",
        "    return dset.map(lambda ex: {\"src_lang\": src_code, \"tgt_lang\": tgt_code},\n",
        "                    desc=\"Adding language columns\")\n",
        "\n",
        "train_with_lang = add_lang_cols(train_norm)\n",
        "val_with_lang   = add_lang_cols(val_norm)\n",
        "\n",
        "def _clean(s: str) -> str:\n",
        "    return (s or \"\").strip()\n",
        "\n",
        "def format_for_sft(batch):\n",
        "    srcs, tgts = batch[\"src_txt\"], batch[\"tgt_txt\"]\n",
        "    slc, tlc   = batch[\"src_lang\"], batch[\"tgt_lang\"]\n",
        "    texts = []\n",
        "    for s, t, sc, tc in zip(srcs, tgts, slc, tlc):\n",
        "        s1, t1 = _clean(s), _clean(t)\n",
        "        if not s1 or not t1:\n",
        "            texts.append(\"\")\n",
        "            continue\n",
        "        txt = MT_PROMPT.format(\n",
        "            src_name = lang_name(sc),\n",
        "            tgt_name = lang_name(tc),\n",
        "            src_code = sc,\n",
        "            src = s1,\n",
        "            tgt = t1,\n",
        "        ) + EOS\n",
        "        texts.append(txt)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "raw_small = DatasetDict({k:v for k,v in {\n",
        "    \"train\": train_with_lang,\n",
        "    \"validation\": val_with_lang,\n",
        "}.items() if v is not None})\n",
        "\n",
        "train_ds = raw_small[\"train\"].map(\n",
        "    format_for_sft, batched=True, remove_columns=raw_small[\"train\"].column_names,\n",
        "    desc=\"[format] train\"\n",
        ").filter(lambda ex: len(ex[\"text\"]) > 0)\n",
        "\n",
        "val_ds = None\n",
        "if \"validation\" in raw_small:\n",
        "    val_ds = raw_small[\"validation\"].map(\n",
        "        format_for_sft, batched=True, remove_columns=raw_small[\"validation\"].column_names,\n",
        "        desc=\"[format] val\"\n",
        "    ).filter(lambda ex: len(ex[\"text\"]) > 0)\n",
        "\n",
        "print(\"Train/Val sizes:\", len(train_ds), (len(val_ds) if val_ds is not None else 0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uab3TFZ6FeY1"
      },
      "source": [
        "# SFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "Ps1EeDVd3NqC",
        "outputId": "d25db2cb-a535-44a2-91a6-125639f58346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val sizes: 62240 3000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1800' max='1946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1800/1946 4:48:24 < 23:25, 0.10 it/s, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.785300</td>\n",
              "      <td>1.842218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.743700</td>\n",
              "      <td>1.832611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.746500</td>\n",
              "      <td>1.830130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.736000</td>\n",
              "      <td>1.828865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.716300</td>\n",
              "      <td>1.828089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.718500</td>\n",
              "      <td>1.826324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.706300</td>\n",
              "      <td>1.827058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.689500</td>\n",
              "      <td>1.826589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.713800</td>\n",
              "      <td>1.827271</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1800, training_loss=1.7281786198086209, metrics={'train_runtime': 17314.1795, 'train_samples_per_second': 7.189, 'train_steps_per_second': 0.112, 'total_flos': 2.334298864783196e+17, 'train_loss': 1.7281786198086209, 'epoch': 1.8499486125385407})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === SFT with TRL + W&B (Qwen2.5-1.5B, QLoRA) — early stopping, best model, resume ===\n",
        "import os, time, wandb, torch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Special tokens\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# W&B\n",
        "os.environ[\"WANDB_PROJECT\"]   = \"euromt\"\n",
        "os.environ[\"WANDB_NAME\"]      = f\"qwen25-1p5b-sft_{len(train_ds)}_{int(time.time())}\"\n",
        "os.environ[\"WANDB_WATCH\"]     = \"false\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
        "os.environ[\"WANDB_SILENT\"]    = \"true\"\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"euromt\",\n",
        "    name=\"qwen25-1p5b-wmt14-de-en-sft\",\n",
        "    config={\n",
        "        \"model\": MODEL_ID,\n",
        "        \"max_seq_len\": 256,\n",
        "        \"use_4bit\": True,\n",
        "        \"lora_r\": 8,\n",
        "        \"train_rows\": len(train_ds),\n",
        "    },\n",
        ")\n",
        "\n",
        "MAX_SEQ    = 256\n",
        "PER_DEV_BS = 64\n",
        "GRAD_ACCUM = 1\n",
        "\n",
        "USE_EPOCHS = True\n",
        "NUM_TRAIN_EPOCHS = 2\n",
        "MAX_STEPS  = -1           # ignored if USE_EPOCHS=True\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    dataset_text_field=\"text\",\n",
        "    response_template=\"Translation:\\n\",\n",
        "    max_seq_length=MAX_SEQ,\n",
        "    packing=False,\n",
        "    args=SFTConfig(\n",
        "        output_dir=\"outputs_sft\",\n",
        "        report_to=[\"wandb\"],\n",
        "\n",
        "        # Batch & steps\n",
        "        per_device_train_batch_size=PER_DEV_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        num_train_epochs=(NUM_TRAIN_EPOCHS if USE_EPOCHS else 1),\n",
        "        max_steps=(MAX_STEPS if not USE_EPOCHS else -1),\n",
        "\n",
        "        # Opt & sched (stabili)\n",
        "        learning_rate=5e-5,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.05,\n",
        "        weight_decay=0.0,\n",
        "        #label_smoothing_factor=0.1,\n",
        "        max_grad_norm=1.0,\n",
        "        optim=\"adamw_8bit\",\n",
        "\n",
        "        # Dataloader\n",
        "        #group_by_length=True,\n",
        "        dataloader_num_workers=2,\n",
        "        dataloader_pin_memory=True,\n",
        "\n",
        "        # Log, eval, save\n",
        "        logging_steps=20,\n",
        "        eval_strategy=\"steps\" if val_ds is not None else \"no\",\n",
        "        eval_steps=200,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        save_total_limit=3,\n",
        "\n",
        "        # Early stopping + best model\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        load_best_model_at_end=True,\n",
        "\n",
        "        seed=3407,\n",
        "    ),\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,\n",
        "            early_stopping_threshold=0.0\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Train/Val sizes:\", len(train_ds), (0 if val_ds is None else len(val_ds)))\n",
        "\n",
        "# Riprendi automaticamente se c'è un checkpoint\n",
        "resume = False\n",
        "trainer.train(resume_from_checkpoint=resume)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Big9OKyepdd5"
      },
      "source": [
        "# Saving best checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgzNdwmGqnA8",
        "outputId": "af0e5781-5b32-4cbd-aa51-31494bfdd351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✔️ Copiato BEST checkpoint in:\n",
            "/content/drive/MyDrive/mt_checkpoints/qwen25_sft_best_20250909_000610\n"
          ]
        }
      ],
      "source": [
        "# === Save on Google Drive the BEST (or LAST) checkpoint ===\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os, shutil, glob, time\n",
        "\n",
        "OUT_DIR = \"outputs_sft\"  # deve coincidere con SFTConfig.output_dir\n",
        "\n",
        "def pick_checkpoint(out_dir):\n",
        "    # 1) try taking best (if load_best_model_at_end=True)\n",
        "    best = getattr(trainer.state, \"best_model_checkpoint\", None)\n",
        "    if best and os.path.isdir(best):\n",
        "        return best, \"best\"\n",
        "    # 2) fallback: last checkpoint saved\n",
        "    ckpts = sorted(glob.glob(os.path.join(out_dir, \"checkpoint-*\")), key=os.path.getmtime)\n",
        "    if ckpts:\n",
        "        return ckpts[-1], \"last\"\n",
        "    return None, None\n",
        "\n",
        "ckpt_path, kind = pick_checkpoint(OUT_DIR)\n",
        "assert ckpt_path is not None, f\"No checkpoint found in: {OUT_DIR}\"\n",
        "\n",
        "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "GDRIVE_DIR = f\"/content/drive/MyDrive/mt_checkpoints/qwen25_sft_{kind}_{stamp}\"\n",
        "\n",
        "shutil.copytree(ckpt_path, GDRIVE_DIR)\n",
        "tokenizer.save_pretrained(GDRIVE_DIR)\n",
        "\n",
        "print(f\"✔️ Copiato {kind.upper()} checkpoint in:\\n{GDRIVE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IV9fcZ60hHe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOq8lbjn2I5VOFOv0Y9TiIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}