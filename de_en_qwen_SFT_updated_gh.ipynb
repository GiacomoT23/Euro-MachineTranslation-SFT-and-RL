{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiacomoT23/Euro-MachineTranslation-SFT-and-RL/blob/main/de_en_qwen_SFT_updated_gh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBJQPZzyHr99"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzQq9SE-Hlvz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-input --upgrade pip\n",
        "!pip install --no-input unsloth bitsandbytes accelerate peft trl sentencepiece protobuf hf_transfer\n",
        "!pip install --no-input transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq3XZU80Hu3M"
      },
      "source": [
        "# Model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499,
          "referenced_widgets": [
            "d254844dcd3545f49ce3ca7675d8f0b5",
            "b3eb5337f65d4becb853cd83cfcfb8ab",
            "6833f046b10e4fb2bdab88689458e8ff",
            "1b51a0e5b4974499ba13c33019669954",
            "a8679460741b474a9362ab6b528c3d6f",
            "52c31779dd474b0e9177cdd8a11a74d1",
            "b27049292029426a955492bc820c0987",
            "8520e0ec0b584b15ae945247fd356a0b",
            "e183fed35064481d80e157974ee0b0f3",
            "c0ad4d2b56584e2685ff7fd5f2b3548c",
            "57c11c1e3c0b43cfa81b2e74af9274d4",
            "d2bba023ddc4417d999f375825229a19",
            "5bf28da49ab640778a38a9582ef65054",
            "39072da95e804ed58bf78431b28a41e5",
            "07d567feb78748fba26b5fa64734d36e",
            "be8b917d3b334b9199e28c70fee89a57",
            "6ec974e82a654011a276115e4d6b047e",
            "cc804a91e4f740b6bf576422cc57ff34",
            "50f7161e3a8b4c198948057bf74ef374",
            "6e729e2a8c7d4018b553870ee40ae115",
            "0ef16bc851be4ac5aca23cdb753f0852",
            "8a687dc50cb743e9adecdfe96f9b55f0",
            "56710667abd34a47aa6c02595b8dd356",
            "86378fd42aca4ccf8894e5456204adf8",
            "e974fb54294e475a9037b62dfebc3c12",
            "c7518d358aa14164a430f09f32f35ece",
            "879bc85cb7194ba3bdf75e784601fef5",
            "00a47a73b64247cc8d2b7b1e811dad61",
            "44f56a0bc77e4ea7a5a4d688d3d46c48",
            "54b45d3329e4436f92f9b2e9e6761e0f",
            "9ff33295b6954d02a2a57b18e3102e35",
            "2cde5871d33b474781d6b773c034c7f8",
            "94fc250b25814e44abbdc988d119c181",
            "86e96a3a12f846b8b1baa85bdfd39a53",
            "e46e22c578004ec2a86a529c2ee74d07",
            "5958c2208c9d49e49db67c7b0db8ff89",
            "1a39cd12a6de45f3a1856f3ea4b071bc",
            "634c728006ce40f8bcf08a97f263fadc",
            "137cc58980824d1b8c6928b7f932a02a",
            "a15e4cf874bb42df865e4360c6b4bf5d",
            "afb1b0504f9f43659ee51ac353037825",
            "3b43bd67c6c64b4daa591876e1b34ae6",
            "3f508f899d474ae2a204817826f0afdc",
            "845d0313785f401ca3f7a63ba14669a1",
            "b7dafa5e4a354ce9b2b5a7465c9cc651",
            "348c4483e5ca4e60bee01e8f4173568e",
            "c881cce557cc40b6906dc975cb0531ab",
            "3efc89dc3e5740b4a4f16e8806fe84f6",
            "48b203e0ab1343dc97440a55926fe4f1",
            "363ffca2d6ad46038d08211fc8efb6ce",
            "4dd2174e022748e7a0c2cabf0ed54920",
            "5d10760033a1496fb25aaadf47788a2b",
            "ef5266ab928a4b57a86e74f0c174cd89",
            "91b33f63f73c4219accfb6aeb5d5df79",
            "70505e1070874662833711de8a593767",
            "65aa201d717e4caa934931c9d4bae00f",
            "d2ac0eeb212346b98ef1c3cb80c6e324",
            "dc4bf88513c948f6a8cc765a4087b001",
            "06e7f37bfb684db2bbc9baa0c1db49e3",
            "f9342621c5a742679398ac4e31e776bb",
            "2e427aa129c94b83a05c1acbe4a1f3d8",
            "0513119c286c429d9a914d6574f0f762",
            "15673b0d534e4eb99325241d1b62592d",
            "9476542d99b24ec89aa3f6eee04ff788",
            "91c6c33805504abd985b98af31b3ebf8",
            "b51ab88069f44cd299d96a7f86ddc3a3",
            "f2f4cb336c8c46aa9a2f1699fb6c0adf",
            "d5bd3651fe494ef58c800a0cfeccdfed",
            "7b119b2880b0450996dfcc0ebb0c0a26",
            "d3ac6d6264be4106a79bd0847f9ea554",
            "d51c1cf418104cd2a3b8a5ea03e8eabc",
            "93203f1898bf44a8b3cb6e9fa6606537",
            "5ae5d1be7afe497eafaf9de06330f982",
            "965677c44eec4ab6a9b6d44e4b7be767",
            "7f78fab1a27e405ea7ca42dfff9548e4",
            "42f2719cde954317868505f1fd78ff36",
            "226dd672c26f4ded919fa543520ec6f6",
            "2a0a0d03c52c4a8ca5b3f428a35760b7",
            "befd61da014546daa94989c047b8a909",
            "da108da5f8f446fca446125c71db533b",
            "65c78257a5114c969df34809b8cce497",
            "a25ae67873ea4509b5f1c4957e95bd92",
            "912e50def8de4677b330c615b4832866",
            "cd4fc632fcdb47cca4e29cf4d4688f16",
            "85c69728adb843c98c9dd0a0eb99bcf3",
            "c40391dfc57e4e0e8b205b31cdc52bb5",
            "4646f8ad68d94a4fbece1be92dfec570",
            "f21073ee3b764994b1e7787afa6164a8"
          ]
        },
        "id": "14YjMw2QHvTR",
        "outputId": "c4d312ad-3088-4f4b-8793-4e9d38d5be99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.5: Fast Qwen2 patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.9.5 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: unsloth/Qwen2.5-3B | 4bit=True | GC=unsloth | seq=256\n",
            "Trainable params (LoRA+heads): ~29,933,568\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch, os\n",
        "\n",
        "USE_4BIT = True\n",
        "USE_GC   = \"unsloth\"\n",
        "MAX_SEQ  = 256\n",
        "MODEL_ID = \"unsloth/Qwen2.5-3B\"\n",
        "\n",
        "dtype = None if USE_4BIT else torch.float16\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = MODEL_ID,\n",
        "    max_seq_length = MAX_SEQ,\n",
        "    dtype          = dtype,\n",
        "    load_in_4bit   = USE_4BIT,\n",
        ")\n",
        "\n",
        "# LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = USE_GC,\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model: {MODEL_ID} | 4bit={USE_4BIT} | GC={USE_GC} | seq={MAX_SEQ}\")\n",
        "print(f\"Trainable params (LoRA+heads): ~{n_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlXY57VDIEde"
      },
      "source": [
        "# Loading splits\n",
        "Loading the preprocessed dataset saved on drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iVVPuajIE2t",
        "outputId": "74374d10-b60e-44cb-8d9a-98c980cfc300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "{'train': 111966, 'validation': 2780, 'test': 3003}\n"
          ]
        }
      ],
      "source": [
        "# === Load filtered dataset from Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from datasets import load_from_disk, DatasetDict\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/mt_datasets/wmt14_de-en_sample150000__filtered__len169_lid95_r2.25_ck60_trainVAL_filtered_testRAW\"\n",
        "ds_proc = load_from_disk(SAVE_DIR)\n",
        "\n",
        "train_filt = ds_proc.get(\"train\")\n",
        "val_filt   = ds_proc.get(\"validation\")\n",
        "test_filt  = ds_proc.get(\"test\")\n",
        "\n",
        "print({k: len(v) for k, v in ds_proc.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1f0qX5UIa68"
      },
      "source": [
        "# Formatting for SFT in unsloth\n",
        "Formatting with system-user-assistant template. Model will be trained only on assistant responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556,
          "referenced_widgets": [
            "9b82c16a00914cf6862c2f848f9c9963",
            "eae6078c06434ca38e3c299078b9eb8b",
            "79d9e10065274705ba69d5ef9fccd599",
            "09b2472c849146b181c9d0aa93741979",
            "23cef6d45a854a11b4f8f2dc07bd212e",
            "e196d39f568847eeb65e5e795e0445ca",
            "227369dcdcf448b5aa0cee50e043558b",
            "2a682b7f6d1142d89493c8c1a832cd67",
            "9392ffa987a1490fa065aeff67fa29d2",
            "e916a56bc4324b54a9eba81213acbf51",
            "f86b6cb5f37d418283ac95acb1d419b5",
            "2d926b0ae4b0452eb19e3e10b05ba628",
            "894b06fcce4945089057eabd77e2dbb0",
            "7e214decb24f4c588994614f29544d0b",
            "d45f08e920554c8eadfde7bcf6eb9769",
            "8c0affa066394011b228fa97ec62f956",
            "0f745de5fb914b47bd00e29bc8934b56",
            "9c1779af16f64999ae1711b6702d8fd8",
            "61aa9a6e1d4a45dea5feb040a3354307",
            "5aff12e9aff042c59ca75b56d9d5c9f5",
            "e8c7175708fc4b56a57f904c71e327c3",
            "d236ade99add4260a8883a3dd8faed09",
            "04ca4828676a41e49314508ff1ee150a",
            "a2c0eae742404ad886b364575335368e",
            "2ee10d37f8fa4ac28d7ae9c66a588bc5",
            "b25776eb72464a19b95b2f760710f7d9",
            "a4cb60b1f4d54491b25c1135575ea663",
            "f1add7bc545b489186975b1d3060c08d",
            "c5965a82d6d341e5833c253eefdd0a39",
            "6e6ec3a479e04a25bf72439931a9da99",
            "272770bcc97142f08ef91bd3ee1060ef",
            "95e67a0bffda4c9ab4a8fdd8795bd37f",
            "94a1b2beb05d44fb87f7be3ba846610b",
            "3be47f9d2fc9456d8709d46b5e7693c6",
            "b319064d4bd54422bd7c2823ea8264c2",
            "c0cd5391308d42668f9715d72ae32afa",
            "6608d8e28d17431aa17c33e1cd379412",
            "cb1facd9cfbd4ee1806b6a478547f37e",
            "809eaf98d27f4bae90d786f8a17d37c0",
            "a83578593468449c8fc0b46e0bb1446a",
            "0dccaa51761c4e8bb16b3262206e2d15",
            "632d8b6819664b67b3bdda13d2c5b9c7",
            "d6642503e81f4bf9a6849fae5b0c597f",
            "b3ac1b9ece4943d684872f15004641b2",
            "c0692ad24e2c4cd3bcac230c8642541e",
            "dbd25078935646bf86003a60e4cd99a8",
            "ebf04924c594414dab6c78a600a978e4",
            "926be7f7902f46a28ac3c6acda834f96",
            "5bd8b3f96acd479695081e36f0b40c7b",
            "c8cbfdac64db48198fcdf65022f459f4",
            "9825d9eafd614c70b5c328178544fe3e",
            "410c0ea0ddf241efaadc9fc12abaa911",
            "021ac9fc96c2422ebc3cb753e6bd305a",
            "047345438656437796d6b046013783a3",
            "ce39948a52bb43ed87f68151e7d28467",
            "14b94ae1cf0043b5a2e86ae836f518d4",
            "b52b76eae67f4ef29b0e09e3b7ff9841",
            "f25e3fedb0d340119d9686fd2ff71f90",
            "d4b090c97a504f0b8f97379ef2d0e1c2",
            "2a90f648cf1245638f073a4afd7e7910",
            "408bb50c3c9a46eeabdbb36a24e0be7e",
            "428f3cbe88954ce99ad26b50c74591b4",
            "6f0b2501af644430aa045aa122c37568",
            "076e01962f874889853a97efc6fa3c2f",
            "30f6306452904b17a72950ccb61c5755",
            "ff65d3d7e91e48b6836163c973cdbe3c",
            "e27809b20bfa4454b124699135c8b65c",
            "1c4cd2dd3ac24012b4038d7e1a8cafc7",
            "8bd3d356d29147e483177e0d437c5eb1",
            "b16dc04bdfd6434a90bbb909aa35aac8",
            "b7b73c237f934ca48304fb5acfdfa629",
            "f34f7ccaf2b84f6d9a010ecfaabd0d6a",
            "7c5893c1353b4ddeabea7398130ea313",
            "6bbedd6efeee42e8b0053eb28c164610",
            "32bd219e44a64f76ab51d0e7a2c3db92",
            "648980bb1ee44abea4e24f889626bd38",
            "e3e60a8b2dbd4fd289185ebdd7caa502"
          ]
        },
        "id": "0PYIqJDTIbMi",
        "outputId": "e8a9eb50-08cb-40eb-c634-d4721ffae073"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[normalize] add src_txt/tgt_txt from translation:   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[normalize] add src_txt/tgt_txt from translation:   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[normalize] add src_txt/tgt_txt from translation:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[chatml] make text (train):   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[chatml] make text (val):   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val sizes: 111966 2780\n",
            "Esempio text:\n",
            " <|im_start|>system\n",
            "You are a translation engine. Translate from German (de) to English (en).<|im_end|>\n",
            "<|im_start|>user\n",
            "Dabei handelt es sich um das Berliner Ãœbereinkommen von 1937 und das StraÃŸburger Ãœbereinkommen von 1973, die beide recht alt, der heutigen Sachlage in Europa nicht mehr angemessen und demnach weitgehend Ã¼berholt sind.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "These are the 1937 Berlin agreement and the 1973 Strasbourg agreement which, because they are quite old, are not relevant to the situation we have in Europe today and have therefore become largely obsolete.<|im_end|>\n",
            "\n",
            "Esempio text:\n",
            " <|im_start|>system\n",
            "You are a translation engine. Translate from German (de) to English (en).<|im_end|>\n",
            "<|im_start|>user\n",
            "Dieser Mix fÃ¼hrte wieder zu lebhaften GesprÃ¤chen unter den Teilnehmern in den Pausen und wÃ¤hrend der Abendveranstaltung, die von allen Beteiligten an dieser Veranstaltung in den Gesamtbeurteilungen als sehr informativ empfunden wurde.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "This mixture, once again, facilitated lively discussions among the participants during the breaks and, of course, during the evening event and was found to be very informative as expressed in the overall feedback survey.<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "import unicodedata\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
        "\n",
        "SRC, TGT = \"de\", \"en\"\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize(\"NFKC\", (s or \"\"))\n",
        "\n",
        "def ensure_src_tgt_cols(dset):\n",
        "    cols = set(dset.column_names)\n",
        "    if {\"src_txt\",\"tgt_txt\"}.issubset(cols):\n",
        "        return dset\n",
        "    elif \"translation\" in cols:\n",
        "        def to_cols(batch):\n",
        "            src = [nfkc(x.get(SRC, \"\")) for x in batch[\"translation\"]]\n",
        "            tgt = [nfkc(x.get(TGT, \"\")) for x in batch[\"translation\"]]\n",
        "            return {\"src_txt\": src, \"tgt_txt\": tgt}\n",
        "        return dset.map(\n",
        "            to_cols, batched=True, batch_size=4096,\n",
        "            desc=\"[normalize] add src_txt/tgt_txt from translation\",\n",
        "        )\n",
        "    else:\n",
        "        raise KeyError(\"Split has not src_txt/tgt_txt or translation.\")\n",
        "\n",
        "train_norm = ensure_src_tgt_cols(train_filt)\n",
        "val_norm   = ensure_src_tgt_cols(val_filt) if val_filt is not None else None\n",
        "test_norm  = ensure_src_tgt_cols(test_filt) if test_filt is not None else None\n",
        "\n",
        "LANG_NAME = {\"de\": \"German\", \"en\": \"English\"}\n",
        "def lang_name(code: str) -> str: return LANG_NAME.get(code.lower(), code.upper())\n",
        "\n",
        "SYSTEM_TMPL = \"You are a translation engine. Translate from {src_name} ({src_code}) to {tgt_name} ({tgt_code}).\"\n",
        "\n",
        "def to_text_chatml(batch, src_code=SRC, tgt_code=TGT):\n",
        "    srcs, tgts = batch[\"src_txt\"], batch[\"tgt_txt\"]\n",
        "    texts = []\n",
        "    sys_msg = SYSTEM_TMPL.format(\n",
        "        src_name=lang_name(src_code), tgt_name=lang_name(tgt_code),\n",
        "        src_code=src_code, tgt_code=tgt_code,\n",
        "    )\n",
        "    for s, t in zip(srcs, tgts):\n",
        "        s1, t1 = (s or \"\").strip(), (t or \"\").strip()\n",
        "        if not s1 or not t1:\n",
        "            texts.append(\"\"); continue\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": sys_msg},\n",
        "            {\"role\": \"user\",   \"content\": s1},\n",
        "            {\"role\": \"assistant\", \"content\": t1},\n",
        "        ]\n",
        "        txt = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(txt)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "train_ds = train_norm.map(\n",
        "    to_text_chatml, batched=True,\n",
        "    remove_columns=train_norm.column_names,\n",
        "    desc=\"[chatml] make text (train)\",\n",
        ").filter(lambda ex: len(ex[\"text\"]) > 0)\n",
        "\n",
        "val_ds = None\n",
        "if val_norm is not None:\n",
        "    val_ds = val_norm.map(\n",
        "        to_text_chatml, batched=True,\n",
        "        remove_columns=val_norm.column_names,\n",
        "        desc=\"[chatml] make text (val)\",\n",
        "    ).filter(lambda ex: len(ex[\"text\"]) > 0)\n",
        "\n",
        "print(\"Train/Val sizes:\", len(train_ds), (0 if val_ds is None else len(val_ds)))\n",
        "print(\"Esempio text:\\n\", train_ds[0][\"text\"])\n",
        "print(\"Esempio text:\\n\", train_ds[1][\"text\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8aiRYQHJ2Et"
      },
      "source": [
        "# SFT\n",
        "SFT parameters are set here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "e3ee9f98a182426cab2dd04b16de567e",
            "350b4943cb4e45f3b7d4538754f3cae2",
            "0617f12c22fd4503a77ed157ffc31d38",
            "f9726c3002844be180f9ca8268b3d998",
            "2bbef3d8d7864a29b727c8d33898721b",
            "558490e7de774ef783a5be17711642ce",
            "63ffe709887844919cd82b8b3e7cc02d",
            "e88e0c38ec9e49caac1f99cf9251f198",
            "8c229981a8f14487ac33a92f094ffc64",
            "1525f2150f6d4a2d95b44459f7068521",
            "98fe063c784d4cc384a410c2471ba53b",
            "e6bb7dea621e4ce3a1a5a96f3f73acb2",
            "145e82bbdec84173b991ea3de7efab2e",
            "f44dafd6604844ba8f7d9e355b863dfc",
            "7fcd0cbb9e924fdaaf4b92527c728b15",
            "ceecd63c55cf434797f89dbf198ed25e",
            "fe4b9aa9e7a24a749ce1b87176b9555f",
            "0e6eede75c4a46ffbe374744c8fa527d",
            "37865f7e6a254ba3993958af7224fae1",
            "9b733a4173b348fc838c1e7af79e17f2",
            "65a98b917c0a4e2b947721faa228090e",
            "53c448d955164cbf9278ed8b1fc2fd25",
            "3a59fb7935cb4b71a729de9474329814",
            "af0830d0ea9143c1ad462bde98becd3d",
            "db7a81cd399443f0b816364a958038c8",
            "56d22aba3ef8470cb82635b553d4ed0e",
            "bc42feb043374949bf903755250a0e68",
            "e15fad4d54ce47a8b8623e1623b1caf8",
            "510a86e480fe400981d639b901dd8cd2",
            "fce2ff3def0d4316a91adc0ab97b095e",
            "d29ff42721f14761b47691335b7137f2",
            "e7950a4b34c3404b86117b000c86ced6",
            "fcf5388cf2534cc49a76622d4216da89",
            "dd6ee4e1313d4732975dfd86fca6d73e",
            "386efbb0acbb42228015fcf56a132925",
            "bb76047cf613436aae218a655d2c5c92",
            "0aa9c67d486a4be9b21db198c777e940",
            "0cd7bdc0a34e4790a94d7f0413af8da4",
            "424d4c34f04a4bac87c4e8902ab3bafc",
            "c8455f440aba4d069b1edc28a6e63b81",
            "e8058c663c9a4d8db8eeab23adf44fce",
            "93ba4b8d5adc44c198240673001b92a1",
            "748fc3cff1fb4cc7a5c4d4a58c694842",
            "64564a23c1e5490d98c638bc3bdf7e3f"
          ]
        },
        "id": "e6CbidiOJvNa",
        "outputId": "8fc4de04-458f-4d80-aaa8-0c6ac72c7a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val sizes: 111966 2780\n",
            "Precision: fp16\n",
            "Ready. Training on assistant spans only (Qwen 2.5 ChatML).\n"
          ]
        }
      ],
      "source": [
        "# === SFT with TRL + W&B (Qwen2.5-3B, QLoRA) â€” ChatML + train_on_responses_only ===\n",
        "import os, time, wandb, torch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import EarlyStoppingCallback\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "# pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# W&B login\n",
        "os.environ[\"WANDB_PROJECT\"]   = \"euromt\"\n",
        "os.environ[\"WANDB_NAME\"]      = f\"qwen25-3b-sft_chatml_{len(train_ds)}_{int(time.time())}\"\n",
        "os.environ[\"WANDB_WATCH\"]     = \"false\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
        "os.environ[\"WANDB_SILENT\"]    = \"true\"\n",
        "if not os.getenv(\"WANDB_API_KEY\"):\n",
        "    from getpass import getpass\n",
        "    os.environ[\"WANDB_API_KEY\"] = getpass(\"W&B API key: \")\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"euromt\",\n",
        "    name=\"qwen25-3b-wmt14-de-en-sft_chatml\",\n",
        "    config={\"model\": MODEL_ID, \"max_seq_len\": 256, \"use_4bit\": True, \"lora_r\": 16, \"train_rows\": len(train_ds)},\n",
        "    settings=wandb.Settings(start_method=\"thread\"),\n",
        ")\n",
        "\n",
        "MAX_SEQ    = 256\n",
        "PER_DEV_BS = 128\n",
        "GRAD_ACCUM = 1\n",
        "\n",
        "use_gpu  = torch.cuda.is_available()\n",
        "name     = torch.cuda.get_device_name(0).lower() if use_gpu else \"\"\n",
        "is_a100  = \"a100\" in name\n",
        "bf16_flag = bool(is_a100)\n",
        "fp16_flag = bool(use_gpu and not is_a100)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_length=MAX_SEQ,\n",
        "    packing=False,\n",
        "    args=SFTConfig(\n",
        "        output_dir=\"outputs_sft\",\n",
        "        report_to=[\"wandb\"],\n",
        "\n",
        "        per_device_train_batch_size=PER_DEV_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        num_train_epochs=2,\n",
        "        max_steps=-1,\n",
        "\n",
        "        learning_rate=1e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.05,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=0.6,\n",
        "        optim=\"adamw_8bit\",\n",
        "\n",
        "        #group_by_length=True,\n",
        "        dataloader_num_workers=2,\n",
        "        dataloader_pin_memory=True,\n",
        "\n",
        "        logging_steps=20,\n",
        "        eval_strategy=\"steps\" if val_ds is not None else \"no\",\n",
        "        eval_steps=150,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=150,\n",
        "        save_total_limit=3,\n",
        "\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        load_best_model_at_end=True,\n",
        "\n",
        "        seed=3407,\n",
        "        bf16=bf16_flag,\n",
        "        fp16=fp16_flag,\n",
        "    ),\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0)],\n",
        ")\n",
        "\n",
        "# Loss only on responses\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|im_start|>system\\n\",\n",
        "    response_part    = \"<|im_start|>assistant\\n\",\n",
        ")\n",
        "\n",
        "print(\"Train/Val sizes:\", len(train_ds), (0 if val_ds is None else len(val_ds)))\n",
        "print(\"Precision:\", \"bf16\" if bf16_flag else (\"fp16\" if fp16_flag else \"fp32\"))\n",
        "print(\"Ready. Training on assistant spans only (Qwen 2.5 ChatML).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "OmS4bWUWebUN",
        "outputId": "987f47e8-def3-4e64-87a5-015747b0b273"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 111,966 | Num Epochs = 2 | Total steps = 1,750\n",
            "O^O/ \\_/ \\    Batch size per device = 128 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (128 x 1 x 1) = 128\n",
            " \"-____-\"     Trainable parameters = 29,933,568 of 3,115,872,256 (0.96% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1400' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1400/1750 1:46:31 < 26:40, 0.22 it/s, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.415100</td>\n",
              "      <td>1.455451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.356200</td>\n",
              "      <td>1.442189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.357200</td>\n",
              "      <td>1.437055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.352200</td>\n",
              "      <td>1.428804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.322900</td>\n",
              "      <td>1.436336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.315700</td>\n",
              "      <td>1.435536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.299000</td>\n",
              "      <td>1.433643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1400, training_loss=1.3641400527954102, metrics={'train_runtime': 6402.4467, 'train_samples_per_second': 34.976, 'train_steps_per_second': 0.273, 'total_flos': 5.810809133829489e+17, 'train_loss': 1.3641400527954102, 'epoch': 1.6})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resume = False\n",
        "trainer.train(resume_from_checkpoint=resume)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXFNFuZgZrwd"
      },
      "source": [
        "# Save checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3DohN09R2IX",
        "outputId": "c23760e9-92dc-4edd-f44b-59226f725d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Copied BEST checkpoint in:\n",
            "/content/drive/MyDrive/mt_checkpoints/qwen25_sft_best_20250914_235740\n"
          ]
        }
      ],
      "source": [
        "# === Save on Google Drive the BEST (or LAST) checkpoint ===\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os, shutil, glob, time\n",
        "\n",
        "OUT_DIR = \"outputs_sft\"  # SFTConfig.output_dir\n",
        "\n",
        "def pick_checkpoint(out_dir):\n",
        "    best = getattr(trainer.state, \"best_model_checkpoint\", None)\n",
        "    if best and os.path.isdir(best):\n",
        "        return best, \"best\"\n",
        "    ckpts = sorted(glob.glob(os.path.join(out_dir, \"checkpoint-*\")), key=os.path.getmtime)\n",
        "    if ckpts:\n",
        "        return ckpts[-1], \"last\"\n",
        "    return None, None\n",
        "\n",
        "ckpt_path, kind = pick_checkpoint(OUT_DIR)\n",
        "assert ckpt_path is not None, f\"No checkpoint found in: {OUT_DIR}\"\n",
        "\n",
        "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "GDRIVE_DIR = f\"/content/drive/MyDrive/mt_checkpoints/qwen25_sft_{kind}_{stamp}\"\n",
        "\n",
        "shutil.copytree(ckpt_path, GDRIVE_DIR)\n",
        "tokenizer.save_pretrained(GDRIVE_DIR)\n",
        "\n",
        "print(f\"Copied {kind.upper()} checkpoint in:\\n{GDRIVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2dCihQtZyFp"
      },
      "source": [
        "# Load + check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655,
          "referenced_widgets": [
            "d9e869fa9b4b47faaadab53367e626dc",
            "6d143723712343f98c4e2a0720f49719",
            "47fad3e802b2425da8a8817d226a4f83",
            "ed0a851881024c7fac1127751cbd1592",
            "bb720cdfeb2044559f32244b7fba553a",
            "d876bba8b7f74b478de7136ac8a56b29",
            "c3e4efb4e3144a33a1d6432127b3d9fe",
            "cdedec57f1db489e9420a4383c4b20bf",
            "62dca52245db43f5b63a338d0a38344d",
            "4f4496572879441e9fcebb4d7424130c",
            "8a54349445f44fd1956e2a397037cfd6",
            "7cdc584cb1d84b13860e6d3c1e268abf",
            "a12bbbb673b14f979fc848bd67935152",
            "6baeda7d186e4072bec4a54ab0f3c386",
            "e70e5c46f915442fa88c887c96cb7174",
            "94674a650e9d4a389d58a75351065597",
            "32afdf0ca2044e7382b21c9c5bccb387",
            "0529a58f22064fbc9d29d2e6cd5c427c",
            "6faac21314d84498b7723def907312d8",
            "64ce5f5c0367446bb53531b5cdb31cc3",
            "8ddb1bbc074342cfa95c2c21e103ecf0",
            "8267c2a4c2fe4e808b3b9a512898d87a",
            "41233b6088034a4bbc381b57cf94fbc1",
            "7a420c2e56074da4b75c12143c9b1e05",
            "6d3517a1b7e24f958f9ab5697b6773e8",
            "49e5d789e1974e70905c3c5c33071bdb",
            "1d5de07b967148a3b2df86319456a795",
            "d2a0fbce81194a2fb6c16eb18e8b42ad",
            "9b4a66b431ff40118daedb4d89860a2d",
            "5245f50702be4bbcae5fd493490a5ee8",
            "4712b5188f2b43f09222610ab1b4cc3c",
            "32a4b5bea0b3481d92d67fb0526b7d40",
            "eee62f7d70c746a1973439d463772060",
            "817d6def1308406b9c1bea13269d9051",
            "921c90ae51254c97a5846797c5695fd4",
            "a324cb08e9cd413cb00dc6ac9914b6ff",
            "0bdb903a86f94b60aad23c5216c922c7",
            "d9793a2e202f4a0e8b0d348e72a6bd5d",
            "fc0ec3139a3445d89c18edc3b70fda70",
            "71b19ab21d234539b418fcc362485bb3",
            "d354b9bd47904a8288d63bbe1f847a95",
            "4c8df0c04ba646da916aa4d8bf2ebebe",
            "52bd933b69184cec954a554159599587",
            "2af564bd314e4e09aeb41de84cff9362",
            "6818b494b0274ba3a16b1149f6bb5328",
            "4f3603d8b552438b8f577b8f4468d74e",
            "600ccd212067485482897307d0ba56fc",
            "9fe5ad328bef4d9da325a0d117e315cf",
            "885014418a2b412c8532b75a5215af80",
            "95c623a54a894c96bfcdcd8c312dba56",
            "a4543b87a34b4dc283ac94d5eb093010",
            "4f1030dfd24947a0ac4b3cd477a913ce",
            "45271bb2c3b2400397792a4c820dca8d",
            "2fe0170e30994f4ca49c4456ab716166",
            "d72d376c5b2345809dd372e4d6085a3f",
            "c27b112af3f142b89920bbfbbf775cb1",
            "db90199e177f4488ae76d7092f2ec8d0",
            "863e430ed7184e699bca85b61cb8c754",
            "7a94482546194b699236721b1add91b5",
            "ae16e8a528ba44b7a1e5014f6d39777d",
            "dad75a1cccbc4f708944f6872d6cd0f8",
            "5e06e98882f7484eb1035a6ff3674117",
            "336e09e477f84a2992dcf3b99e76b751",
            "b92d9391c9d342eaa271ba4063352967",
            "9a4675e4f16b4a55a819839793decdcd",
            "2994921e530b4fdab75cac355490f799",
            "09b00e1b8ca94e8994766ca72b9433da",
            "c60f94718e4c4d58ac7202d9b050e3b7",
            "18b1db9d4d0f4b60ae5b9bd51d98cbac",
            "122f9d03221c4755bbd71e6832e2c1e4",
            "6ccf914ee894407ba0603d5a982def16",
            "53a89a191b1b4154b51fa7c8b5e8d6e5",
            "5327e68516254eabbeb382ad106a1aa6",
            "3381c961157d40e9b20bf292de42fdb9",
            "2c431bcbda7d46b6a65ec77b84c17590",
            "f78afec3c60f443ea1edcad4f9c7bad2",
            "996d8da29d85493e9bb1107d4f277d5e",
            "d95911adaa18420abbf9e1980b55a465",
            "151eb20107fa42a2b884f72e0c581edd",
            "7b7f289c3e8b478db67c07740e891f87",
            "3a89a9af1b7041de9ac083faf1297152",
            "f501bad02a1544ac953ec900d91a74de",
            "6c9b899c89084edbbe0c85818e51b97e",
            "908c9cfc85ab463fae2a14c475d8776e",
            "c4bf15e558e24ca38ae57b8f0bcc2647",
            "fc0277f1eb0144618bec58a2977b0c7b",
            "5501a40fa56e48c692177655622df410",
            "1caccdc130cb4bd19d4ff70e883f326d"
          ]
        },
        "id": "qfy3rWDfx4Tc",
        "outputId": "5b42b3b9-527f-4dbd-da42-caae30aafacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Mounted at /content/drive\n",
            "Drive mounted.\n",
            "âœ“ CKPT su Drive: /content/drive/MyDrive/mt_checkpoints/qwen25_sft_best_20250914_235740\n",
            "==((====))==  Unsloth 2025.9.5: Fast Qwen2 patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is equal False\n",
            "Splits: {'train': 111966, 'validation': 2780, 'test': 3003}\n",
            "\n",
            "=== Check ===\n",
            "PeftModel?             True\n",
            "Adapter dir            /content/drive/MyDrive/mt_checkpoints/qwen25_sft_best_20250914_235740\n",
            "Files adapter presenti True True\n",
            "Tensori LoRA           504\n",
            "Layer types            Counter({'Linear4bit': 237})\n",
            "\n",
            " Ready: model/dataset loaded.\n"
          ]
        }
      ],
      "source": [
        "# === LOAD: MOUNT + FIND CKPT + LOAD MODEL & DATA (Qwen2.5-3B, 4-bit) ===\n",
        "# 0) Unsloth\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# 1) Google Drive\n",
        "import os, glob, time, shutil\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.ismount(\"/content/drive\"):\n",
        "        drive.mount(\"/content/drive\")\n",
        "        print(\"Drive mounted.\")\n",
        "    else:\n",
        "        print(\"Drive already mounted.\")\n",
        "except Exception as e:\n",
        "    print(\"Colab not available:\", e)\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL_ID      = \"unsloth/Qwen2.5-3B\"\n",
        "MAX_SEQ       = 256\n",
        "USE_4BIT      = True\n",
        "SRC, TGT      = \"de\", \"en\"\n",
        "DATASET_DIR   = \"/content/drive/MyDrive/mt_datasets/wmt14_de-en_sample150000__filtered__len169_lid95_r2.25_ck60_trainVAL_filtered_testRAW\"\n",
        "CKPT_ROOT     = \"/content/drive/MyDrive/mt_checkpoints\"\n",
        "\n",
        "SPLIT               = \"test\"\n",
        "MAX_EXAMPLES        = None\n",
        "OUT_DIR             = \"/content/drive/MyDrive/mt_eval/qwen3BevalCOMET\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# 2) Find ckpt LoRA on Drive\n",
        "def find_adapter_dirs(root):\n",
        "    hits = []\n",
        "    if os.path.isdir(root):\n",
        "        for p in glob.glob(os.path.join(root, \"**\", \"adapter_model.safetensors\"), recursive=True):\n",
        "            adir = os.path.dirname(p)\n",
        "            mtime = os.path.getmtime(p)\n",
        "            hits.append((mtime, adir))\n",
        "    return sorted(hits, reverse=True)\n",
        "\n",
        "hits = find_adapter_dirs(CKPT_ROOT)\n",
        "if hits:\n",
        "    CKPT_DIR = hits[0][1]\n",
        "    print(\"CKPT su Drive:\", CKPT_DIR)\n",
        "else:\n",
        "    print(\"No adapter in\", CKPT_ROOT)\n",
        "    raise FileNotFoundError(\"No adapter_model.safetensors found.\")\n",
        "\n",
        "# 3) Load\n",
        "import torch, bitsandbytes as bnb\n",
        "from collections import Counter\n",
        "from peft import PeftModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_from_disk, DatasetDict, load_dataset\n",
        "\n",
        "dtype = None if USE_4BIT else torch.float16\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = MODEL_ID,\n",
        "    max_seq_length = MAX_SEQ,\n",
        "    dtype          = dtype,\n",
        "    load_in_4bit   = USE_4BIT,\n",
        ")\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Pad was None\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f\"Is equal {tokenizer.pad_token==tokenizer.eos_token}\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, CKPT_DIR)\n",
        "FastLanguageModel.for_inference(model)\n",
        "model.eval()\n",
        "\n",
        "# 4) Load dataset\n",
        "def nfkc(s: str):\n",
        "    return (s or \"\").strip()\n",
        "\n",
        "def project_to_src_tgt(dset, src_code=SRC, tgt_code=TGT):\n",
        "    cols = set(dset.column_names)\n",
        "    if {\"src_txt\",\"tgt_txt\"}.issubset(cols):\n",
        "        return dset\n",
        "    elif \"translation\" in cols:\n",
        "        def to_cols(batch):\n",
        "            src = [nfkc(ex.get(src_code, \"\")) for ex in batch[\"translation\"]]\n",
        "            tgt = [nfkc(ex.get(tgt_code, \"\")) for ex in batch[\"translation\"]]\n",
        "            return {\"src_txt\": src, \"tgt_txt\": tgt}\n",
        "        return dset.map(to_cols, batched=True, desc=\"Project translation â†’ src/tgt\")\n",
        "    else:\n",
        "        raise ValueError(f\"Split not recognized: {cols}\")\n",
        "\n",
        "ds_eval = DatasetDict()\n",
        "if os.path.isdir(DATASET_DIR):\n",
        "    ds_proc = load_from_disk(DATASET_DIR)\n",
        "    for sp in (\"train\",\"validation\",\"test\"):\n",
        "        if sp in ds_proc:\n",
        "            ds_eval[sp] = project_to_src_tgt(ds_proc[sp])\n",
        "else:\n",
        "    raw = load_dataset(\"wmt14\", \"de-en\")\n",
        "    ds_eval[\"test\"] = project_to_src_tgt(raw[\"test\"])\n",
        "\n",
        "print(\"Splits:\", {k: len(v) for k, v in ds_eval.items()})\n",
        "\n",
        "# 5) Check\n",
        "print(\"\\n=== Check ===\")\n",
        "print(\"PeftModel?            \", isinstance(model, PeftModel))\n",
        "print(\"Adapter dir           \", CKPT_DIR)\n",
        "print(\"Files adapter presenti\",\n",
        "      os.path.isfile(os.path.join(CKPT_DIR, \"adapter_model.safetensors\")),\n",
        "      os.path.isfile(os.path.join(CKPT_DIR, \"adapter_config.json\")))\n",
        "lora_params = [n for n,_ in model.named_parameters() if \"lora_\" in n]\n",
        "print(\"Tensori LoRA          \", len(lora_params))\n",
        "mods = [m for m in model.modules() if isinstance(m, (bnb.nn.Linear4bit, bnb.nn.Linear8bitLt))]\n",
        "print(\"Layer types           \", Counter(type(m).__name__ for m in mods))\n",
        "\n",
        "# 6) Variables\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LANG_NAME = {\"de\":\"German\",\"en\":\"English\"}\n",
        "def lang_name(c): return LANG_NAME.get(c.lower(), c.upper())\n",
        "SYSTEM_TMPL = \"You are a translation engine. Translate from {src_name} ({src_code}) to {tgt_name} ({tgt_code}).\"\n",
        "SYSTEM_TEXT = SYSTEM_TMPL.format(\n",
        "    src_name=lang_name(SRC), src_code=SRC, tgt_name=lang_name(TGT), tgt_code=TGT\n",
        ")\n",
        "print(\"\\n Ready: model/dataset loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlU0jIASY0Ja",
        "outputId": "4687d6b2-f74b-4007-ac6c-499a0dd0e07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BATCH] 3003 examples, bs=512\n",
            "inizio batch\n",
            "inizio batch\n",
            "inizio batch\n",
            "inizio batch\n",
            "inizio batch\n",
            "inizio batch\n",
            "[3003/3003]\n",
            "Done in 265.7s\n",
            "Saved: /content/drive/MyDrive/mt_eval/qwen3BevalCOMET/gen_de-en_test_20250916-001201.csv\n",
            "\n",
            "[0] SRC: Gutach: Noch mehr Sicherheit fÃ¼r FuÃŸgÃ¤nger\n",
            "HYP: Assessor: More safety for pedestrians.\n",
            "REF: Gutach: Increased safety for pedestrians\n",
            "\n",
            "[1] SRC: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-FuÃŸgÃ¤ngerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der Ã¤lteren Rathausampel.\n",
            "HYP: They are only 100 metres apart: on Tuesday, the new B 33 pedestrian crossing was opened in Gutach, in sight of the older Rathaus crossing.\n",
            "REF: They are not even 100 metres apart: On Tuesday, the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights.\n",
            "\n",
            "[2] SRC: Zwei Anlagen so nah beieinander: Absicht oder SchildbÃ¼rgerstreich?\n",
            "HYP: Two factories so close together: purpose or a deliberate act of vandalism?\n",
            "REF: Two sets of lights so close to one another: intentional or just a silly error?\n"
          ]
        }
      ],
      "source": [
        "# === PREDICT  ===\n",
        "import os, time, re, torch, pandas as pd\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Config\n",
        "SPLIT=\"test\"; BATCH_SIZE=512; MAX_EXAMPLES=None; MAX_NEW_TOKENS=128; DO_SAMPLE=False; PRINT_EVERY=200\n",
        "tok = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
        "tok.padding_side=\"left\"\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "tok.padding_side=\"left\"\n",
        "def build_messages(src):\n",
        "    return [{\"role\":\"system\",\"content\":SYSTEM_TEXT},{\"role\":\"user\",\"content\":src.strip()}]\n",
        "# Dati\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dset = ds_eval[SPLIT]\n",
        "take = len(dset) if MAX_EXAMPLES is None else min(MAX_EXAMPLES,len(dset))\n",
        "sources = dset.select(range(take))[\"src_txt\"]; refs = dset.select(range(take))[\"tgt_txt\"]\n",
        "# Stop & ban\n",
        "im_end = tok.convert_tokens_to_ids(\"<|im_end|>\"); eos = tok.eos_token_id\n",
        "im_start = tok.convert_tokens_to_ids(\"<|im_start|>\")\n",
        "eos_ids = [t for t in (im_end, eos) if t is not None]\n",
        "# Postprocess\n",
        "MARKERS = (\"<|im_end|>\",\"<|endoftext|>\",\"<|im_start|>\",\"åˆå§‹åŒ–\",\"å§‹åŒ–\")\n",
        "def clean(txt):\n",
        "    p = min([txt.find(m) for m in MARKERS if m in txt] + [len(txt)])\n",
        "    txt = txt[:p]\n",
        "    if \"\\n\" in txt: txt = txt.split(\"\\n\",1)[0]\n",
        "    m = re.search(r\"^(.{1,300}?[\\.!\\?])( |$)\", txt)\n",
        "    return (m.group(1) if m else txt).replace(\"\\uFFFD\",\"\").strip()\n",
        "print(f\"[BATCH] {take} examples, bs={BATCH_SIZE}\")\n",
        "t0=time.time(); hyps=[]\n",
        "with torch.inference_mode():\n",
        "    for i in range(0, take, BATCH_SIZE):\n",
        "        print(\"inizio batch\")\n",
        "        tok.padding_side=\"left\"\n",
        "        batch = sources[i:i+BATCH_SIZE]\n",
        "        prompts = [tok.apply_chat_template(build_messages(s), tokenize=False, add_generation_prompt=True) for s in batch]\n",
        "        tok.padding_side=\"left\"\n",
        "        enc = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ).to(dev)\n",
        "        tok.padding_side=\"left\"\n",
        "        cut = enc[\"input_ids\"].shape[1]\n",
        "        tok.padding_side=\"left\"\n",
        "        outs = model.generate(\n",
        "            input_ids=enc[\"input_ids\"], attention_mask=enc.get(\"attention_mask\"),\n",
        "            max_new_tokens=MAX_NEW_TOKENS, do_sample=DO_SAMPLE, eos_token_id=eos_ids,\n",
        "            pad_token_id=tok.pad_token_id, early_stopping=True, use_cache=True,\n",
        "        )\n",
        "        tok.padding_side=\"left\"\n",
        "        for j in range(outs.shape[0]):\n",
        "            tok.padding_side=\"left\"\n",
        "            gen = outs[j, cut:]\n",
        "            hyps.append(clean(tok.decode(gen, skip_special_tokens=False)))\n",
        "        if (i+BATCH_SIZE) % PRINT_EVERY==0 or i+BATCH_SIZE>=take: print(f\"[{min(i+BATCH_SIZE,take)}/{take}]\")\n",
        "        tok.padding_side=\"left\"\n",
        "print(f\"Done in {time.time()-t0:.1f}s\")\n",
        "# Save csv and print preview\n",
        "stamp=time.strftime(\"%Y%m%d-%H%M%S\"); csv_path=os.path.join(OUT_DIR, f\"gen_de-en_{SPLIT}_{stamp}.csv\")\n",
        "pd.DataFrame({\"src\":sources, \"hyp\":hyps, \"ref\":refs}).to_csv(csv_path, index=False)\n",
        "print(\"Saved:\", csv_path)\n",
        "for k in range(min(3,len(hyps))):\n",
        "    print(f\"\\n[{k}] SRC: {sources[k]}\\nHYP: {hyps[k]}\\nREF: {refs[k]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU8ez1vH-Y_h"
      },
      "source": [
        "# COMET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617,
          "referenced_widgets": [
            "4f9cf0c065a14830bfcc83d94e96d121",
            "9d9106d904e9457d941125bfb95917b1",
            "5d488be6e2ef475cb9c60b0ec71f60fc",
            "41da9d7bd372472cb8122dc5ac437c81",
            "38c10cff5ac941549bad4f1bfe365dbd",
            "ac40efb7ebcb48728574f25ba9e9a7b8",
            "7f2caeca948d48fdb31623d77ada0424",
            "f83ad5ad6b8143beace4898481ba148b",
            "3b7c2c2c9dca4531bf9b63b6808233fb",
            "7ac0f7b8801940a6853f3e677c38e566",
            "8ebb1aa1b818457887e97a5e0ef79b3d",
            "99d00694d26f458cb24c019e68738977",
            "9b8e57e5a2234f2db005f9ed9306770e",
            "2c9370efeaa44b2cabbc43472d1513fc",
            "640961564f2f49f0a3f8bd8fd08529d9",
            "5e970540e93c4fe9ad86c57fb59c43fc",
            "0dd6175517c04b1099db7751adfae5df",
            "491b4108f5a04ede91e3d950be19a5be",
            "259cffca00234625b8366b482d75f204",
            "8688a0dd5f234a8db68b79ef2fde2736",
            "a4676e00c49d420491dcc81e2bc3f285",
            "126690de72274ed384eac88e893da0c7",
            "da17d5f0e9424c5a83e578bbf77d21c5",
            "84e790e20abc4c1c91b2777ad902f20b",
            "098b4ed99b734e9c8f3f59dd615fa561",
            "854f33eafa2048d4bb1fea421133c5b7",
            "2d606c03fb4849aa99dff7c0543c952a",
            "37c53b2482ec4d9aa22c6c825b2938db",
            "b58bfb3250cb4b6ca4a374b5d5946a08",
            "924dfdfc96264a199a6083467617d7cb",
            "a6f8c3f364a94905b4bda685d650bf08",
            "b11fe83a56964fc9b67d347703b41ae9",
            "d49c94de83b94c5fa4cfb710fc30b0c2",
            "65183268c62743e3824a6261b32abfb4",
            "f1ee5b26c4ed4efbb08704e8af9cab5f",
            "8678c5c4325341b8adabdbfccb17cd0a",
            "12b7d4af830d46118745b00c6927f55a",
            "db840423b8f84d239a07a235741f1273",
            "c192952649244d89862ad367a5109e73",
            "7e0ba02ffab44010b117cf18fc895cfd",
            "69f300cec8004f548d88d4bb36cf9672",
            "680e0d736f3e4092b0f69adee3c6f7c3",
            "521b6d22aa214b5f8766f3e2ae1bcea9",
            "5b1b5350c38b4646a74d98eb382138de",
            "421c68bc205046f28870f125d0ba6ec5",
            "7addd65677104ae6a9fadb367f2cab70",
            "95e29ad22bd84519a05115425fdb0612",
            "29baa45fea8140fca9e76962ad46afaf",
            "5afd45453d1645e589f47e31c9c8c8f9",
            "f2a53dd3314d45789857d8260528d8cf",
            "9c9546fe69be404c8d93e78213f9158a",
            "a7bcb92b86184ad9838d3cd0612647cd",
            "be932ae20f8f4f3fa5faae0431c46e34",
            "63fa0371865c4b17a58d7b5249c51dab",
            "4da15a3048ea4121b90262d8263a344e",
            "c6204d252834419e93552ee3b3700769",
            "b142525675354c4cacccaf573981a55f",
            "fbe2a018e7e64e0bad6a4efc626676a9",
            "b17af609939b4b10a118c521cff8172e",
            "509408350ec3420082e498fe7f00d3d7",
            "d07cb977781c4600b353f313c9d78edc",
            "ba3ea95897b44d3aa4f102667e8abec5",
            "a8938bc946e44405b8079b62bd81f3da",
            "4cab3b727051403d92ba7060458e6edf",
            "f264e15d7cf746fbb7f69e1770242e22",
            "97641dd430324664902f597032acea6e",
            "549c1bf17aba4e14b616ce3f2b0804e0",
            "7ec5fa90b5eb4eb39759315067a07f3b",
            "e379337517e343a9a4fdddc9e239cb6b",
            "8439125de6a54ff8a23ee38163aca003",
            "934fbe0f5f5f443cb5f06bcb742f949f",
            "9ae15e26d96b46539668603e43e7a180",
            "ee86a322573341b29982a502d7f6ea6d",
            "8881ebb811e7473383ff903d3c449679",
            "94f7068c5466488daebd1272f98702e5",
            "839575ef8ff34485be8dd3cb6bfce487",
            "1f616c471bbd4debb0f5a77df396d224",
            "c6f840b92c7d437cb4b41147bfadadb7",
            "69fdee962f6f445a862f6042ba3f133a",
            "4de8105e1f89447dae1460f6de16eb0b",
            "212ffe029cf0421eac1fdf0333d5ce25",
            "2ba96e0c2d164218b2fb707abc8a5558",
            "25db472396be4069bcd559f5e8a85238",
            "c3540baaba444943906e476974a7969d",
            "39ee4f605f114c10b44e339532b0cd19",
            "0a8267bc0dd7434aba46c9b96e4106c1",
            "25db3431fa564cc4b5ff82c8e9b7218c",
            "387986766c204b0e94898b73d6ee22f8",
            "1aeddbc3e45f4b82a003b590a3dd82b6",
            "a60dcd1fa7fd4945890c62d3aba0725a",
            "396196d52b0f42f2bd077aede6620f4d",
            "3f2967975f0f49e0b8ffdfb970b34d92",
            "62c9f0aa5dd745a8b04e092f489de518",
            "820169cd2c1d4aec988eed67c9c6a7f5",
            "1e6ce911a6864c97875927972c69ae13",
            "660acc7bd18e4a52ab633ad0f584e9f5",
            "9a422de4566841b4b3a5bb9f81dc8c6d",
            "5b01eb52f60f4a809cd9ab3609d03861",
            "fa8b7c2169ae4200ac40e5c74561a7e5",
            "258f0f78202d43459ba9d7e9fdde816a",
            "825f5475ccd7481ea5d6a986c8bfd622",
            "85b83175fa244647baddeeb3d4e2cfc6",
            "6c8c8c78622b4aa78ec8ca69c5eb817a",
            "03f05a13a8e44ce096b68808a29f9e02",
            "91446336ae134cc9a7917ab6d3cfdead",
            "73aa3decdaac4faab874feff1e3e7332",
            "5be5a07c482448188074a99735f2fc5c",
            "db1653b2362d4d1eb350837177fefb70",
            "0ab5fd6fd9c84ef9bcdf6bdd97ab8c93",
            "29f26962e37f444497e2d9cadb4b331b"
          ]
        },
        "id": "knMGbxsg8vSw",
        "outputId": "b12987dc-b47b-405e-ec51-68f8c5173eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted.\n",
            "CSV found: /content/drive/MyDrive/mt_eval/qwen3BevalCOMET/gen_de-en_test_20250916-001201.csv\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "hparams.yaml:   0%|          | 0.00/567 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "LICENSE: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "checkpoints/model.ckpt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
            "INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:16<00:00,  2.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction({'scores': [0.8927652835845947, 0.7647638320922852, 0.6845570206642151, 0.8375985622406006, 0.8480094075202942, 0.8370135426521301, 0.8407938480377197, 0.8248785138130188, 0.8547746539115906, 0.8231543898582458, 0.8439663052558899, 0.8647288680076599, 0.8259732723236084, 0.763944149017334, 0.7457390427589417, 0.8799477219581604, 0.8735669255256653, 0.8060024976730347, 0.9095523953437805, 0.8526321053504944, 0.8335078954696655, 0.9200155138969421, 0.8423763513565063, 0.8477757573127747, 0.8154047131538391, 0.8102858662605286, 0.8019640445709229, 0.7497314214706421, 0.7094261050224304, 0.6986905932426453, 0.7038756608963013, 0.8170358538627625, 0.8121245503425598, 0.7542086243629456, 0.7542465329170227, 0.7662191987037659, 0.5550954937934875, 0.892902672290802, 0.840292751789093, 0.9509729743003845, 0.8627979159355164, 0.887315034866333, 0.882462203502655, 0.855476975440979, 0.8113126754760742, 0.9200817942619324, 0.8407248854637146, 0.8477080464363098, 0.8272284865379333, 0.9589688181877136, 0.8069164156913757, 0.9227167963981628, 0.8828017115592957, 0.8369560837745667, 0.747025728225708, 0.725271999835968, 0.8225603103637695, 0.8790860772132874, 0.9148591756820679, 0.8913639187812805, 0.8726544976234436, 0.8443524837493896, 0.8489721417427063, 0.859969437122345, 0.8883071541786194, 0.8487895131111145, 0.7997883558273315, 0.8550223708152771, 0.9012711644172668, 0.9143571257591248, 0.8163098692893982, 0.820405900478363, 0.8917255401611328, 0.9159427285194397, 0.973639190196991, 0.9368782639503479, 0.9025381207466125, 0.9127358794212341, 0.9818413257598877, 0.9065116047859192, 0.9159206748008728, 0.8300604224205017, 0.8735694289207458, 0.884425938129425, 0.8586334586143494, 0.8069047927856445, 0.9295653700828552, 0.8579339981079102, 0.8622862100601196, 0.8619699478149414, 0.8894289135932922, 0.9041112661361694, 0.9384157657623291, 0.9460644125938416, 0.9499786496162415, 0.9160451292991638, 0.810893714427948, 0.8492997884750366, 0.8816813826560974, 0.8715385794639587, 0.9013640880584717, 0.8939723968505859, 0.8390251994132996, 0.9227203726768494, 0.8007237911224365, 0.8623484969139099, 0.9780104756355286, 0.7959563732147217, 0.6784385442733765, 0.8727105855941772, 0.7144566178321838, 0.8100985884666443, 0.9658049941062927, 0.8239365816116333, 0.8532235026359558, 0.6858559250831604, 0.8992888331413269, 0.5709267258644104, 0.6762538552284241, 0.581818163394928, 0.719920814037323, 0.8623118996620178, 0.8683620691299438, 0.8548946976661682, 0.9176360964775085, 0.8138269782066345, 0.8307585716247559, 0.7983601689338684, 0.777633011341095, 0.754055380821228, 0.8922597169876099, 0.800214946269989, 0.7553769946098328, 0.9012373685836792, 0.943869411945343, 0.9341300129890442, 0.7745590806007385, 0.8746823668479919, 0.885649561882019, 0.8505492806434631, 0.8427396416664124, 0.852980375289917, 0.6341822147369385, 0.7539103627204895, 0.6710789799690247, 0.9054331183433533, 0.9118391871452332, 0.8000449538230896, 0.7649343609809875, 0.6777673363685608, 0.7327650189399719, 0.8775015473365784, 0.9073594212532043, 0.7601831555366516, 0.8258516788482666, 0.9059327840805054, 0.819360077381134, 0.8744996190071106, 0.7535824775695801, 0.8033334016799927, 0.8710454106330872, 0.9100548028945923, 0.8407776951789856, 0.8709002137184143, 0.9057124257087708, 0.6613340973854065, 0.827032744884491, 0.83296799659729, 0.9559024572372437, 0.9089938402175903, 0.8811524510383606, 0.9065840840339661, 0.8400467038154602, 0.8688771724700928, 0.8869109749794006, 0.9527105093002319, 0.935735285282135, 0.9176862239837646, 0.8582096695899963, 0.8047698140144348, 0.884330689907074, 0.7963963150978088, 0.8867602348327637, 0.8913798928260803, 0.9031687378883362, 0.8801997900009155, 0.8782888054847717, 0.8903533816337585, 0.9066783785820007, 0.9346309304237366, 0.915715754032135, 0.9203304648399353, 0.9062763452529907, 0.9192551970481873, 0.768821656703949, 0.7625589966773987, 0.7965799570083618, 0.8154059052467346, 0.8975048661231995, 0.7064239382743835, 0.7846826314926147, 0.8532688021659851, 0.6093083620071411, 0.5665991902351379, 0.5245408415794373, 0.7267065644264221, 0.887448251247406, 0.7031871676445007, 0.8683684468269348, 0.8597033619880676, 0.8096850514411926, 0.7271050810813904, 0.8987511396408081, 0.9060555100440979, 0.7323392033576965, 0.751096785068512, 0.7800127863883972, 0.6129772663116455, 0.8965844511985779, 0.7698829770088196, 0.6106427311897278, 0.8388726115226746, 0.824944794178009, 0.7024653553962708, 0.910033643245697, 0.8767760396003723, 0.859041154384613, 0.8194534778594971, 0.8149146437644958, 0.7625576853752136, 0.7609313726425171, 0.8080734610557556, 0.7814437747001648, 0.8011243343353271, 0.8664578199386597, 0.8284048438072205, 0.8552340865135193, 0.8107792139053345, 0.7703487277030945, 0.7418273091316223, 0.822375476360321, 0.8699744939804077, 0.9244366884231567, 0.7090072631835938, 0.761265218257904, 0.8126470446586609, 0.7549625635147095, 0.8364941477775574, 0.8061125874519348, 0.900118887424469, 0.835426390171051, 0.48443517088890076, 0.889868438243866, 0.7507505416870117, 0.92339688539505, 0.8996703028678894, 0.8284289836883545, 0.872648298740387, 0.9110479950904846, 0.8114126324653625, 0.8907491564750671, 0.9045501351356506, 0.8265461921691895, 0.8431299328804016, 0.9071666598320007, 0.877441942691803, 0.8393809199333191, 0.9023173451423645, 0.9510614275932312, 0.8470478057861328, 0.8859676122665405, 0.9405242204666138, 0.8951291441917419, 0.9737268686294556, 0.8122313022613525, 0.873439610004425, 0.8078337907791138, 0.8434948921203613, 0.9186217784881592, 0.9665300846099854, 0.9093655943870544, 0.7860238552093506, 0.8771211504936218, 0.9188464283943176, 0.962748110294342, 0.844836413860321, 0.7944821715354919, 0.932998776435852, 0.9293750524520874, 0.86562579870224, 0.8912181258201599, 0.856732189655304, 0.8473921418190002, 0.5322827696800232, 0.9267157912254333, 0.7333160042762756, 0.8106285929679871, 0.8202521800994873, 0.9115543365478516, 0.7521882653236389, 0.8942334055900574, 0.8228009343147278, 0.7803649306297302, 0.9452910423278809, 0.8907351493835449, 0.8574976325035095, 0.8311969637870789, 0.9546152353286743, 0.9023281931877136, 0.9115571975708008, 0.9234737753868103, 0.8897668719291687, 0.8487153649330139, 0.875963032245636, 0.9555839896202087, 0.874883234500885, 0.9051327109336853, 0.8973740935325623, 0.8902432322502136, 0.8671890497207642, 0.7954010367393494, 0.8433578610420227, 0.8964971303939819, 0.8523775339126587, 0.8384062647819519, 0.8654623031616211, 0.9298728108406067, 0.823131263256073, 0.8991613984107971, 0.7910920977592468, 0.8601143956184387, 0.8946467041969299, 0.7345728874206543, 0.9497955441474915, 0.7721042633056641, 0.8983803391456604, 0.8578551411628723, 0.8843463063240051, 0.9022852778434753, 0.9086770415306091, 0.6992046236991882, 0.4658132791519165, 0.8870911002159119, 0.8472548723220825, 0.8268030285835266, 0.9122404456138611, 0.7778075337409973, 0.7375834584236145, 0.8270941376686096, 0.8507779836654663, 0.8449774384498596, 0.8872955441474915, 0.7710858583450317, 0.9172196984291077, 0.9009479880332947, 0.9160251021385193, 0.9042014479637146, 0.7042580246925354, 0.7765321135520935, 0.7989754676818848, 0.9263262152671814, 0.9095302224159241, 0.8378475904464722, 0.8252192139625549, 0.8891122937202454, 0.8819612860679626, 0.8704738616943359, 0.8894701600074768, 0.9016153216362, 0.3925810754299164, 0.7039196491241455, 0.43464937806129456, 0.9138631820678711, 0.8319264054298401, 0.7857818603515625, 0.8228033185005188, 0.8864961266517639, 0.9054808020591736, 0.8836265802383423, 0.8543587327003479, 0.8677474856376648, 0.8344010710716248, 0.8461171984672546, 0.8812245726585388, 0.9082254767417908, 0.964103102684021, 0.9391701817512512, 0.8674775958061218, 0.924258291721344, 0.9343559145927429, 0.9026390910148621, 0.8787456154823303, 0.9117683172225952, 0.8465272784233093, 0.8770216107368469, 0.9092770218849182, 0.8464398384094238, 0.681898295879364, 0.8518305420875549, 0.7841563820838928, 0.9111296534538269, 0.8850285410881042, 0.7858408689498901, 0.4660484492778778, 0.8810330033302307, 0.8573194742202759, 0.8997603058815002, 0.8829456567764282, 0.8519548773765564, 0.9285035133361816, 0.9262515306472778, 0.8972371220588684, 0.9023679494857788, 0.8686828017234802, 0.9020588397979736, 0.8495075106620789, 0.8805409073829651, 0.8722084760665894, 0.8418377041816711, 0.8352290391921997, 0.8967450857162476, 0.8826335072517395, 0.85246741771698, 0.8186716437339783, 0.8617340922355652, 0.8977896571159363, 0.927123486995697, 0.9128393530845642, 0.9297237396240234, 0.8944929242134094, 0.8254000544548035, 0.8832794427871704, 0.915285050868988, 0.926874041557312, 0.9082456827163696, 0.8720743656158447, 0.7671664357185364, 0.8261996507644653, 0.8650729656219482, 0.9172289967536926, 0.9394795894622803, 0.8794209361076355, 0.843051552772522, 0.9232489466667175, 0.9298416972160339, 0.9523652195930481, 0.8678275346755981, 0.9042516350746155, 0.8963266015052795, 0.8374165296554565, 0.7819517254829407, 0.7727100849151611, 0.9301691651344299, 0.8534665107727051, 0.9177038073539734, 0.9649186134338379, 0.8585582971572876, 0.8205499053001404, 0.8578985333442688, 0.8556767106056213, 0.7937576770782471, 0.816188633441925, 0.8573334813117981, 0.9340136647224426, 0.9380921721458435, 0.806480348110199, 0.8380287885665894, 0.913094699382782, 0.9406808018684387, 0.8605920076370239, 0.8705910444259644, 0.7700000405311584, 0.8662894368171692, 0.8744181990623474, 0.8720031380653381, 0.9080840945243835, 0.9071317911148071, 0.8794493079185486, 0.8320040702819824, 0.9055165648460388, 0.8799961805343628, 0.8209311366081238, 0.7827898263931274, 0.8522666692733765, 0.8246431946754456, 0.9014307856559753, 0.8805573582649231, 0.8351998925209045, 0.8360548615455627, 0.8078319430351257, 0.8415892124176025, 0.8699678778648376, 0.9112443327903748, 0.9164526462554932, 0.8238254189491272, 0.7975472807884216, 0.9367044568061829, 0.8034692406654358, 0.8533886075019836, 0.8736499547958374, 0.8418529629707336, 0.8737117648124695, 0.8602635860443115, 0.8971783518791199, 0.9331216216087341, 0.9200916886329651, 0.8587747812271118, 0.8317583799362183, 0.7157934308052063, 0.6912209391593933, 0.77958083152771, 0.5041258931159973, 0.7779161334037781, 0.5252939462661743, 0.8175690174102783, 0.9163901209831238, 0.8480604887008667, 0.9420227408409119, 0.8759285807609558, 0.8156692385673523, 0.8542924523353577, 0.7560603022575378, 0.8621859550476074, 0.8074982762336731, 0.8440241813659668, 0.8381449580192566, 0.8741556406021118, 0.8332069516181946, 0.8208626508712769, 0.6842051148414612, 0.7468090653419495, 0.8503797650337219, 0.8203349709510803, 0.7991877198219299, 0.8059009909629822, 0.8695753812789917, 0.7494975924491882, 0.8871555328369141, 0.9150123596191406, 0.824617862701416, 0.8871592283248901, 0.8820679783821106, 0.8417438864707947, 0.7721726894378662, 0.7117703557014465, 0.9005150198936462, 0.8228207230567932, 0.6707797646522522, 0.9295113682746887, 0.8057234883308411, 0.8681781888008118, 0.7885867953300476, 0.8779304027557373, 0.7781368494033813, 0.8655343055725098, 0.8113846182823181, 0.7808358073234558, 0.7363525032997131, 0.8365792632102966, 0.8865601420402527, 0.8251555562019348, 0.8925548195838928, 0.8544909358024597, 0.9302195310592651, 0.8291875720024109, 0.8607226014137268, 0.8512361645698547, 0.7525832056999207, 0.840415894985199, 0.8533585071563721, 0.8091794848442078, 0.8277568221092224, 0.9302849173545837, 0.7030499577522278, 0.905476987361908, 0.897894561290741, 0.8284255862236023, 0.6321158409118652, 0.7454946637153625, 0.8582848906517029, 0.8005743622779846, 0.780587911605835, 0.6234036684036255, 0.866155743598938, 0.7863048911094666, 0.8828326463699341, 0.7209963202476501, 0.8631640672683716, 0.6826301217079163, 0.7339594960212708, 0.7998471260070801, 0.8441665768623352, 0.7416688799858093, 0.8024750351905823, 0.7678804993629456, 0.7863861918449402, 0.9774165749549866, 0.7952733635902405, 0.6780701279640198, 0.8998603820800781, 0.8052152991294861, 0.7188666462898254, 0.7290183901786804, 0.8746405839920044, 0.9850342869758606, 0.6994523406028748, 0.7870273590087891, 0.7933326363563538, 0.8714076280593872, 0.8069520592689514, 0.8953329920768738, 0.9423626065254211, 0.8251820802688599, 0.9059068560600281, 0.9280759692192078, 0.9005292057991028, 0.8393064141273499, 0.8982402682304382, 0.8140641450881958, 0.8679117560386658, 0.8313570618629456, 0.8986001014709473, 0.8658654093742371, 0.8948413133621216, 0.8303273320198059, 0.8629248738288879, 0.8336547017097473, 0.8992316126823425, 0.8341450095176697, 0.8786260485649109, 0.8466300368309021, 0.6073817014694214, 0.7691611051559448, 0.6598908305168152, 0.708979070186615, 0.874215841293335, 0.7636322975158691, 0.7700853943824768, 0.8981539607048035, 0.8321390151977539, 0.7862905859947205, 0.7707115411758423, 0.8350225687026978, 0.7677038908004761, 0.7467318773269653, 0.8397640585899353, 0.7955195307731628, 0.905548095703125, 0.8894351720809937, 0.7697963118553162, 0.8526713252067566, 0.8562284111976624, 0.8217507004737854, 0.7293128967285156, 0.8541289567947388, 0.7656351923942566, 0.825658917427063, 0.8991546034812927, 0.8762180805206299, 0.796993613243103, 0.85256427526474, 0.9457376003265381, 0.7552298307418823, 0.807260274887085, 0.8757571578025818, 0.923312783241272, 0.8105131983757019, 0.8301591277122498, 0.90565425157547, 0.9189282655715942, 0.6352726221084595, 0.8288143277168274, 0.8272607922554016, 0.610199511051178, 0.8536649346351624, 0.776782214641571, 0.8534319996833801, 0.7400938272476196, 0.8762908577919006, 0.8203069567680359, 0.7325593829154968, 0.8867785334587097, 0.7270675301551819, 0.820469856262207, 0.8571945428848267, 0.8101685643196106, 0.5566978454589844, 0.800997793674469, 0.8980191349983215, 0.49377188086509705, 0.7860633730888367, 0.8163635730743408, 0.8595816493034363, 0.6343874335289001, 0.7197637557983398, 0.7432544827461243, 0.6813692450523376, 0.8905906081199646, 0.8383443355560303, 0.9011878371238708, 0.9496235847473145, 0.8742102384567261, 0.8060774803161621, 0.7813588380813599, 0.9129305481910706, 0.8448439240455627, 0.8014723658561707, 0.6815298199653625, 0.2521073520183563, 0.8296980857849121, 0.8771253824234009, 0.6954774260520935, 0.6913537979125977, 0.7608581185340881, 0.7215036153793335, 0.7978465557098389, 0.9372732043266296, 0.8425197601318359, 0.6479777097702026, 0.9441158175468445, 0.9152652621269226, 0.7416495084762573, 0.6776939034461975, 0.8766223788261414, 0.9305466413497925, 0.908536970615387, 0.7457820773124695, 0.8070955872535706, 0.8494424819946289, 0.8907826542854309, 0.8401054739952087, 0.8902621865272522, 0.6316579580307007, 0.9625656604766846, 0.8821972012519836, 0.8504524827003479, 0.7573870420455933, 0.7283042073249817, 0.8567697405815125, 0.8164666295051575, 0.8225207328796387, 0.833267092704773, 0.8757016062736511, 0.48843684792518616, 0.45678675174713135, 0.7970718741416931, 0.77602219581604, 0.9138506650924683, 0.842329204082489, 0.8818705081939697, 0.5870709419250488, 0.8338752388954163, 0.7598713040351868, 0.7140364050865173, 0.9077878594398499, 0.7330896854400635, 0.7917037010192871, 0.8200480341911316, 0.8155329823493958, 0.8620434403419495, 0.6396098136901855, 0.7154223918914795, 0.94023597240448, 0.7190422415733337, 0.8150888085365295, 0.6443554162979126, 0.7685895562171936, 0.31347140669822693, 0.9342727661132812, 0.6888062953948975, 0.8995559215545654, 0.5256087183952332, 0.6804922223091125, 0.9001066088676453, 0.8723002076148987, 0.7850522398948669, 0.7623554468154907, 0.8074357509613037, 0.8463301062583923, 0.9044415354728699, 0.8438681960105896, 0.8253957629203796, 0.6283029913902283, 0.6865842938423157, 0.9829110503196716, 0.736141562461853, 0.7982388138771057, 0.766368567943573, 0.8144218921661377, 0.7969444394111633, 0.6988151669502258, 0.6917290687561035, 0.7452005743980408, 0.7338737845420837, 0.9003710150718689, 0.8120099306106567, 0.9120330214500427, 0.8762735724449158, 0.8441826701164246, 0.8951314091682434, 0.8982855081558228, 0.8733105063438416, 0.8916158080101013, 0.8467158079147339, 0.7760729193687439, 0.8657035231590271, 0.8952782154083252, 0.9017934203147888, 0.7302658557891846, 0.6134406924247742, 0.858137309551239, 0.6883067488670349, 0.8418859839439392, 0.7845804691314697, 0.6263170838356018, 0.6882627606391907, 0.9050256609916687, 0.8976404070854187, 0.8291060924530029, 0.9373169541358948, 0.8448302745819092, 0.8970349431037903, 0.9005656242370605, 0.9440801739692688, 0.9266306757926941, 0.8657724261283875, 0.9100071787834167, 0.883461594581604, 0.8867705464363098, 0.9039481282234192, 0.9536190629005432, 0.8469735980033875, 0.9573318958282471, 0.908536970615387, 0.9254274964332581, 0.8862711191177368, 0.9355877041816711, 0.8398423790931702, 0.8984885811805725, 0.91782146692276, 0.9570117592811584, 0.860223114490509, 0.8445319533348083, 0.904736340045929, 0.8995264172554016, 0.8753296732902527, 0.8700298070907593, 0.8882927298545837, 0.8840915560722351, 0.918816328048706, 0.8909583687782288, 0.9241188168525696, 0.9513759016990662, 0.8512850403785706, 0.9360657930374146, 0.8792473673820496, 0.9169755578041077, 0.9398783445358276, 0.8582684397697449, 0.8258266448974609, 0.8624338507652283, 0.8672225475311279, 0.9210103154182434, 0.6927070021629333, 0.8666214346885681, 0.79082852602005, 0.7567972540855408, 0.7274707555770874, 0.7571708559989929, 0.7646339535713196, 0.7655271887779236, 0.7583793997764587, 0.5548961758613586, 0.8183332681655884, 0.8291699290275574, 0.871615469455719, 0.6913515329360962, 0.7796016931533813, 0.6899650692939758, 0.7851556539535522, 0.7008681297302246, 0.789847731590271, 0.8884792923927307, 0.7493402361869812, 0.7593263983726501, 0.5261601209640503, 0.6915650963783264, 0.8921239972114563, 0.8495768904685974, 0.820511519908905, 0.7728837132453918, 0.850073516368866, 0.8753877282142639, 0.6730009317398071, 0.773857057094574, 0.7735316157341003, 0.643535315990448, 0.8151655197143555, 0.7792333364486694, 0.8442806601524353, 0.8599517941474915, 0.7361152768135071, 0.8267894387245178, 0.9062342047691345, 0.8182457089424133, 0.9043272733688354, 0.8400024771690369, 0.8737847208976746, 0.9205950498580933, 0.8810057044029236, 0.7950799465179443, 0.6378008723258972, 0.7655094265937805, 0.8825133442878723, 0.640717089176178, 0.9423008561134338, 0.8121467232704163, 0.7921774387359619, 0.9011932611465454, 0.8118976950645447, 0.9272745251655579, 0.8284311294555664, 0.7169553637504578, 0.8843705058097839, 0.7463061809539795, 0.8412547707557678, 0.7969585061073303, 0.8711283206939697, 0.8309794068336487, 0.844315230846405, 0.8003115653991699, 0.6747731566429138, 0.7679264545440674, 0.8201586604118347, 0.8937034606933594, 0.7858189940452576, 0.8345931768417358, 0.7467848658561707, 0.9045915603637695, 0.8308084011077881, 0.6892892718315125, 0.8279732465744019, 0.8666579127311707, 0.38011351227760315, 0.7973756194114685, 0.8543512225151062, 0.8139367699623108, 0.9119647145271301, 0.9040994644165039, 0.6426810622215271, 0.9079270958900452, 0.777459979057312, 0.6259525418281555, 0.8712332248687744, 0.8126415610313416, 0.7582294344902039, 0.7613933682441711, 0.8971720933914185, 0.7320575714111328, 0.9003191590309143, 0.8471000790596008, 0.884023904800415, 0.7619733214378357, 0.8412172198295593, 0.7345718741416931, 0.8732942938804626, 0.822191059589386, 0.809729278087616, 0.834651529788971, 0.8915650248527527, 0.9055038094520569, 0.8463110327720642, 0.9564433693885803, 0.748477578163147, 0.8749305009841919, 0.7937111854553223, 0.9757305383682251, 0.8090131878852844, 0.8970251679420471, 0.8707704544067383, 0.8387927412986755, 0.8866782188415527, 0.7807826399803162, 0.9049134850502014, 0.7281602025032043, 0.8298954367637634, 0.8796800971031189, 0.770512580871582, 0.791485607624054, 0.714225709438324, 0.9147022366523743, 0.8780054450035095, 0.898855447769165, 0.9134721159934998, 0.863997757434845, 0.8531784415245056, 0.9434072971343994, 0.7701113820075989, 0.8849737644195557, 0.8342471718788147, 0.8596620559692383, 0.9539548754692078, 0.9050188660621643, 0.8981121778488159, 0.8743409514427185, 0.8926702737808228, 0.8115060925483704, 0.8548581004142761, 0.7301012277603149, 0.8767134547233582, 0.8546246886253357, 0.6934375166893005, 0.8334023356437683, 0.7622532248497009, 0.8574028015136719, 0.6971308588981628, 0.9058776497840881, 0.8317274451255798, 0.7955263257026672, 0.8629133105278015, 0.8696154952049255, 0.7745510339736938, 0.820858895778656, 0.9586092233657837, 0.8137488961219788, 0.8516918420791626, 0.8629310727119446, 0.9228933453559875, 0.8514818549156189, 0.860710084438324, 0.8146556615829468, 0.7719903588294983, 0.8888203501701355, 0.9133663177490234, 0.9374191164970398, 0.9253284335136414, 0.9004362225532532, 0.8823367953300476, 0.8992887139320374, 0.9163485169410706, 0.8468123078346252, 0.8839674592018127, 0.8379009962081909, 0.8511549234390259, 0.8458088040351868, 0.8225970268249512, 0.8748673796653748, 0.8469979763031006, 0.8716986775398254, 0.7904191613197327, 0.960395872592926, 0.7738751769065857, 0.9165845513343811, 0.9222437739372253, 0.8349443078041077, 0.8856746554374695, 0.6717569231987, 0.8532148003578186, 0.8247336745262146, 0.8585529327392578, 0.8464293479919434, 0.8232008814811707, 0.8524037599563599, 0.8025320172309875, 0.8548751473426819, 0.8904611468315125, 0.8463653326034546, 0.7681344747543335, 0.9350118041038513, 0.6210206151008606, 0.7513545751571655, 0.7524879574775696, 0.8552888631820679, 0.8004045486450195, 0.876600444316864, 0.8593729138374329, 0.8862612843513489, 0.8614825010299683, 0.8366828560829163, 0.8967893719673157, 0.8453962802886963, 0.8377018570899963, 0.8474277853965759, 0.8086367249488831, 0.9207656383514404, 0.91361004114151, 0.7191829085350037, 0.9329994320869446, 0.9641045928001404, 0.6879397630691528, 0.8424490690231323, 0.8739609122276306, 0.8758106827735901, 0.8327285647392273, 0.832184910774231, 0.8713445067405701, 0.8797888159751892, 0.7457755208015442, 0.9361262917518616, 0.9561759829521179, 0.8959181904792786, 0.7982054352760315, 0.7074775695800781, 0.8425859808921814, 0.7552880048751831, 0.6072707772254944, 0.8797938227653503, 0.9018123149871826, 0.9470345377922058, 0.8557448983192444, 0.8386296629905701, 0.8824365139007568, 0.8851162791252136, 0.8942943215370178, 0.7323077321052551, 0.7893984913825989, 0.6910820603370667, 0.8391880393028259, 0.9191402792930603, 0.9178388714790344, 0.8407110571861267, 0.810883641242981, 0.7595289349555969, 0.929821252822876, 0.8777333498001099, 0.8071264624595642, 0.8691580891609192, 0.8007124066352844, 0.8166458606719971, 0.8306369185447693, 0.8034422993659973, 0.8797301054000854, 0.8866518139839172, 0.8607444167137146, 0.8367080092430115, 0.8663150668144226, 0.8534778952598572, 0.8949574828147888, 0.7954074740409851, 0.840296745300293, 0.8719207048416138, 0.9252082705497742, 0.917255163192749, 0.8986327648162842, 0.9269967675209045, 0.34760570526123047, 0.9016426205635071, 0.35053572058677673, 0.7062260508537292, 0.8704840540885925, 0.8701337575912476, 0.920252799987793, 0.9183042645454407, 0.8169103264808655, 0.9787899851799011, 0.7802973985671997, 0.9815860986709595, 0.9863113760948181, 0.9628927707672119, 0.9501727819442749, 0.8590599298477173, 0.8505915403366089, 0.8739383816719055, 0.8393611311912537, 0.9232482314109802, 0.728477418422699, 0.8791463971138, 0.8563607335090637, 0.941363513469696, 0.9490386843681335, 0.9276376366615295, 0.7692784667015076, 0.653971254825592, 0.977202296257019, 0.8745843768119812, 0.7881990075111389, 0.8955771923065186, 0.39147883653640747, 0.7022247910499573, 0.8406457901000977, 0.9094827771186829, 0.8442494869232178, 0.9074528217315674, 0.8782719373703003, 0.7913796305656433, 0.8398076295852661, 0.8922130465507507, 0.8272605538368225, 0.9098549485206604, 0.7643643617630005, 0.9410645365715027, 0.9335033297538757, 0.9380953907966614, 0.9274111390113831, 0.9371618032455444, 0.8979907631874084, 0.9344250559806824, 0.9231703877449036, 0.8312223553657532, 0.9611732363700867, 0.9006052613258362, 0.9372084736824036, 0.9418846964836121, 0.9412898421287537, 0.8765752911567688, 0.8887741565704346, 0.8693013787269592, 0.8439573049545288, 0.8941037058830261, 0.9293087720870972, 0.8594903349876404, 0.8545979857444763, 0.8817148804664612, 0.8849440813064575, 0.8434741497039795, 0.8825059533119202, 0.862926185131073, 0.9407312273979187, 0.9471824765205383, 0.9535213112831116, 0.8872975707054138, 0.9446080327033997, 0.8243239521980286, 0.9168991446495056, 0.8797566294670105, 0.8505311012268066, 0.8657784461975098, 0.7966213226318359, 0.6826069355010986, 0.9226258993148804, 0.8778825402259827, 0.7675119638442993, 0.7227396368980408, 0.5940812826156616, 0.8257129788398743, 0.8844851851463318, 0.731647789478302, 0.47800740599632263, 0.7911708950996399, 0.5744460225105286, 0.8354717493057251, 0.8297582864761353, 0.8759393095970154, 0.8770810961723328, 0.8761880993843079, 0.9242910742759705, 0.7890948057174683, 0.7723872065544128, 0.8635698556900024, 0.7032300233840942, 0.9033607840538025, 0.7821888327598572, 0.7952509522438049, 0.8843421339988708, 0.9402148127555847, 0.8549531102180481, 0.853132426738739, 0.8468025326728821, 0.6342674493789673, 0.9194998741149902, 0.9792507290840149, 0.9469383955001831, 0.9088003039360046, 0.9209992289543152, 0.9369423389434814, 0.9359318017959595, 0.8172218203544617, 0.9513240456581116, 0.949224054813385, 0.9262697100639343, 0.8791330456733704, 0.9231950044631958, 0.8237525820732117, 0.9385160803794861, 0.9221001267433167, 0.9099074602127075, 0.9388838410377502, 0.9265618920326233, 0.888576090335846, 0.8875676989555359, 0.872570812702179, 0.9124074578285217, 0.8606773614883423, 0.8646889328956604, 0.9361699223518372, 0.9281567931175232, 0.8833164572715759, 0.9071356654167175, 0.9326306581497192, 0.8238584399223328, 0.8208540081977844, 0.8449921011924744, 0.9826801419258118, 0.8395763039588928, 0.4200212359428406, 0.920366108417511, 0.8915407061576843, 0.8968672156333923, 0.8881049156188965, 0.9242047667503357, 0.9052765965461731, 0.8659133315086365, 0.9192288517951965, 0.9099103808403015, 0.8406231999397278, 0.8991479873657227, 0.8635618090629578, 0.848356306552887, 0.9086015820503235, 0.9058173298835754, 0.841442883014679, 0.9149894118309021, 0.8668519854545593, 0.9463774561882019, 0.9025262594223022, 0.775338351726532, 0.8329746723175049, 0.8887054324150085, 0.8271706700325012, 0.653830349445343, 0.8730366826057434, 0.9235521554946899, 0.8692978620529175, 0.8765087723731995, 0.8765836358070374, 0.8570966124534607, 0.8915508389472961, 0.8726966381072998, 0.8670197129249573, 0.9167439341545105, 0.8553267121315002, 0.9067846536636353, 0.9267838001251221, 0.9602051377296448, 0.8294212222099304, 0.9132285714149475, 0.9428321123123169, 0.9360107183456421, 0.9098427891731262, 0.9104152917861938, 0.8670386672019958, 0.8738653659820557, 0.8634414672851562, 0.8309571146965027, 0.9623667001724243, 0.90022212266922, 0.9247748255729675, 0.8993151187896729, 0.9596940279006958, 0.8825764060020447, 0.8762056827545166, 0.9545957446098328, 0.2925218939781189, 0.9196941256523132, 0.846828043460846, 0.9372323751449585, 0.8678081631660461, 0.8634390234947205, 0.9025812149047852, 0.827106773853302, 0.8764656186103821, 0.8744556307792664, 0.7694111466407776, 0.8602201342582703, 0.9022837281227112, 0.8993468880653381, 0.7890467047691345, 0.8145624399185181, 0.8242518305778503, 0.8446253538131714, 0.8196524977684021, 0.9306403994560242, 0.8997089266777039, 0.927862823009491, 0.8150969743728638, 0.4878888428211212, 0.7209676504135132, 0.8554266691207886, 0.8272322416305542, 0.8028536438941956, 0.8038507103919983, 0.772754967212677, 0.6963211894035339, 0.7399993538856506, 0.8661801218986511, 0.7226029634475708, 0.8933307528495789, 0.8377780318260193, 0.8490265011787415, 0.9286797642707825, 0.9486173391342163, 0.7963312268257141, 0.8253150582313538, 0.8175092935562134, 0.7252755761146545, 0.8088348507881165, 0.8736522793769836, 0.724240243434906, 0.678098201751709, 0.5459460616111755, 0.7543686628341675, 0.8925007581710815, 0.8617797493934631, 0.879234790802002, 0.6961176991462708, 0.818753182888031, 0.7064851522445679, 0.8119057416915894, 0.8410916924476624, 0.7026358246803284, 0.8278283476829529, 0.8331249952316284, 0.8328705430030823, 0.7752918004989624, 0.8206795454025269, 0.8844690322875977, 0.8455386161804199, 0.85948246717453, 0.8223175406455994, 0.8277129530906677, 0.8744646310806274, 0.7543699741363525, 0.6524094939231873, 0.7244096398353577, 0.6167886853218079, 0.7969943284988403, 0.8828146457672119, 0.7270058989524841, 0.925452470779419, 0.9170071482658386, 0.864162027835846, 0.8819046020507812, 0.9103703498840332, 0.859380304813385, 0.865315854549408, 0.8499449491500854, 0.7414713501930237, 0.750870406627655, 0.7101311683654785, 0.5524835586547852, 0.928510308265686, 0.8521997928619385, 0.8770745396614075, 0.8709773421287537, 0.9212343096733093, 0.8990481495857239, 0.8314539790153503, 0.8497670292854309, 0.8800595998764038, 0.8658387064933777, 0.9160459637641907, 0.8680480718612671, 0.8438400626182556, 0.9206672310829163, 0.8870933055877686, 0.9335724711418152, 0.9392852187156677, 0.8354396820068359, 0.8004973530769348, 0.7790800929069519, 0.8928349614143372, 0.7313736081123352, 0.9257969260215759, 0.8059667348861694, 0.7252914905548096, 0.806885302066803, 0.8462646007537842, 0.8564790487289429, 0.87071293592453, 0.8396340012550354, 0.7000871300697327, 0.9416049122810364, 0.9058342576026917, 0.8086553812026978, 0.8486573100090027, 0.8545876741409302, 0.9495884776115417, 0.890618622303009, 0.8244032263755798, 0.9268916249275208, 0.8081831932067871, 0.8127291202545166, 0.8365052938461304, 0.9031849503517151, 0.906434953212738, 0.9203318953514099, 0.9125465750694275, 0.8664515614509583, 0.8384958505630493, 0.7851983308792114, 0.7786616086959839, 0.7579323053359985, 0.642966628074646, 0.8827376961708069, 0.7388532161712646, 0.9131178259849548, 0.8888494968414307, 0.8820958733558655, 0.8727568984031677, 0.9399256110191345, 0.8864896893501282, 0.9164043068885803, 0.6743361949920654, 0.6994397044181824, 0.8800688982009888, 0.8688775897026062, 0.8832772970199585, 0.9733112454414368, 0.7830842137336731, 0.9074693322181702, 0.6651120781898499, 0.8259304165840149, 0.9517708420753479, 0.8848976492881775, 0.8837022185325623, 0.8105002045631409, 0.8402352929115295, 0.8837187886238098, 0.8844137191772461, 0.9674668908119202, 0.8619310259819031, 0.8452797532081604, 0.5539466738700867, 0.9032139182090759, 0.8855370879173279, 0.7821434140205383, 0.8241778612136841, 0.8991778492927551, 0.899328887462616, 0.7554945349693298, 0.8994076251983643, 0.9372960329055786, 0.8052984476089478, 0.9226233959197998, 0.8248311877250671, 0.8326687812805176, 0.8599995374679565, 0.7834073901176453, 0.8702638745307922, 0.8285078406333923, 0.880760669708252, 0.8418186902999878, 0.9183315634727478, 0.8489252328872681, 0.8659246563911438, 0.8942580223083496, 0.8622510433197021, 0.9125407338142395, 0.9185904264450073, 0.9204866290092468, 0.9181365966796875, 0.8757338523864746, 0.8855063915252686, 0.7575111985206604, 0.7375152111053467, 0.8716585040092468, 0.6297464966773987, 0.7736610770225525, 0.6037828326225281, 0.8945615887641907, 0.5732599496841431, 0.8278998136520386, 0.5002694129943848, 0.6732240915298462, 0.6334089636802673, 0.8650243878364563, 0.6697496175765991, 0.7237301468849182, 0.9295188784599304, 0.7125136852264404, 0.9146506786346436, 0.4101594388484955, 0.7644845247268677, 0.8920490145683289, 0.9343673586845398, 0.8797255158424377, 0.9049866795539856, 0.7904635071754456, 0.8145558834075928, 0.838757336139679, 0.9337844252586365, 0.8991222977638245, 0.7485817670822144, 0.9195040464401245, 0.7737540602684021, 0.7007065415382385, 0.9775322079658508, 0.7726357579231262, 0.5511994361877441, 0.8054364919662476, 0.8642649054527283, 0.8227954506874084, 0.9228952527046204, 0.8584043979644775, 0.900211751461029, 0.7696884870529175, 0.7259289622306824, 0.8624916672706604, 0.8436999917030334, 0.9471603035926819, 0.7848949432373047, 0.7987442016601562, 0.8842291235923767, 0.849850594997406, 0.9009681344032288, 0.8634243011474609, 0.7969362735748291, 0.7772443294525146, 0.8860129714012146, 0.9105560183525085, 0.8735846877098083, 0.8949269652366638, 0.9489057660102844, 0.8831225037574768, 0.888680636882782, 0.9320011734962463, 0.838134229183197, 0.7543683052062988, 0.8924643397331238, 0.9466069340705872, 0.81803297996521, 0.7809354662895203, 0.8824012875556946, 0.9517576694488525, 0.8763272166252136, 0.8716941475868225, 0.8690858483314514, 0.833021879196167, 0.8145553469657898, 0.851248562335968, 0.8410274386405945, 0.8125527501106262, 0.8605361580848694, 0.7426947951316833, 0.7882448434829712, 0.9649286866188049, 0.8940644860267639, 0.9097586274147034, 0.884108304977417, 0.9105110764503479, 0.8470322489738464, 0.9364035725593567, 0.8590871095657349, 0.8535640239715576, 0.8608223795890808, 0.7749419808387756, 0.8077367544174194, 0.807845950126648, 0.8762629628181458, 0.8156960010528564, 0.8242337703704834, 0.8282838463783264, 0.8228333592414856, 0.8852167129516602, 0.8084016442298889, 0.8116063475608826, 0.9072578549385071, 0.6560170650482178, 0.8676689267158508, 0.9119860529899597, 0.8730164170265198, 0.8930384516716003, 0.8420271277427673, 0.9416125416755676, 0.862880289554596, 0.8746386170387268, 0.9100357890129089, 0.8759398460388184, 0.8112627267837524, 0.9494146704673767, 0.9088758230209351, 0.9026321768760681, 0.910610556602478, 0.8833025097846985, 0.8898816704750061, 0.8001485466957092, 0.9311695694923401, 0.8497920632362366, 0.9331686496734619, 0.8909642696380615, 0.8638999462127686, 0.8907837271690369, 0.8035197854042053, 0.8617709875106812, 0.857610285282135, 0.8869151473045349, 0.8432648777961731, 0.8464967012405396, 0.8328713178634644, 0.708860456943512, 0.7481008172035217, 0.8710440993309021, 0.8271415829658508, 0.8840011954307556, 0.8667234778404236, 0.8856298327445984, 0.8415131568908691, 0.9709703326225281, 0.9231792092323303, 0.8322256207466125, 0.9373586177825928, 0.8630385994911194, 0.8726857304573059, 0.9058420062065125, 0.8412793278694153, 0.8912865519523621, 0.8635688424110413, 0.8697922229766846, 0.8069860339164734, 0.877814769744873, 0.8400658369064331, 0.8946976661682129, 0.9115732312202454, 0.7802267670631409, 0.9257270693778992, 0.9015468955039978, 0.8177238702774048, 0.875132143497467, 0.9136170744895935, 0.8495209813117981, 0.8674371242523193, 0.8839370608329773, 0.7323200106620789, 0.7439468502998352, 0.9441182017326355, 0.8755030632019043, 0.9446884989738464, 0.8684970140457153, 0.898637592792511, 0.8604428768157959, 0.9483814835548401, 0.9221601486206055, 0.9538698792457581, 0.9311456084251404, 0.942146360874176, 0.8773223757743835, 0.8943802118301392, 0.6759966015815735, 0.8799203038215637, 0.883327841758728, 0.893690288066864, 0.92872554063797, 0.9047884345054626, 0.9365283846855164, 0.9173298478126526, 0.8869056105613708, 0.8225892186164856, 0.8467058539390564, 0.927492082118988, 0.9669530391693115, 0.7941222786903381, 0.943929135799408, 0.9100127816200256, 0.8467026352882385, 0.9523612856864929, 0.815413236618042, 0.9143900275230408, 0.9253227710723877, 0.9178566932678223, 0.8782872557640076, 0.8991293907165527, 0.9280813932418823, 0.9413579106330872, 0.9287644624710083, 0.7895984053611755, 0.8526156544685364, 0.8904873132705688, 0.8704306483268738, 0.9270921349525452, 0.8711238503456116, 0.8598803877830505, 0.9469529390335083, 0.9120641350746155, 0.7722187638282776, 0.8776493668556213, 0.7840033173561096, 0.8328538537025452, 0.8704070448875427, 0.8277050852775574, 0.8057813048362732, 0.838634192943573, 0.8059030175209045, 0.8558987975120544, 0.7610788941383362, 0.8528798222541809, 0.8317248225212097, 0.8933915495872498, 0.8865346312522888, 0.8582368493080139, 0.8503347635269165, 0.8677407503128052, 0.8392606377601624, 0.8901762366294861, 0.9383147358894348, 0.934507429599762, 0.8223980665206909, 0.8307749629020691, 0.8939037919044495, 0.9000866413116455, 0.7855122685432434, 0.7098307013511658, 0.8627206683158875, 0.8334823846817017, 0.8178519606590271, 0.880975067615509, 0.8146265149116516, 0.8458198308944702, 0.799685001373291, 0.8453056216239929, 0.8176171779632568, 0.9127316474914551, 0.923915445804596, 0.939755380153656, 0.8918849229812622, 0.8497766852378845, 0.9151908159255981, 0.9014979004859924, 0.8715566992759705, 0.8744528889656067, 0.8403524160385132, 0.964087188243866, 0.9315133690834045, 0.8213610649108887, 0.928788959980011, 0.8603523373603821, 0.914130687713623, 0.9026851058006287, 0.9593263864517212, 0.9605817794799805, 0.8389537334442139, 0.8619681596755981, 0.809219241142273, 0.8400550484657288, 0.8752425312995911, 0.776573896408081, 0.8433398604393005, 0.8939847946166992, 0.8410036563873291, 0.8334025144577026, 0.8260654211044312, 0.9572005867958069, 0.8972447514533997, 0.9455859661102295, 0.9041061997413635, 0.8874878287315369, 0.8356785774230957, 0.9335450530052185, 0.8977843523025513, 0.8734092116355896, 0.8101325631141663, 0.8311194777488708, 0.8334912061691284, 0.8088955879211426, 0.8866949677467346, 0.8975154161453247, 0.873336136341095, 0.8224548697471619, 0.8606205582618713, 0.8825944066047668, 0.8538901209831238, 0.9397637248039246, 0.9105318784713745, 0.8990615010261536, 0.9301095604896545, 0.9204436540603638, 0.9480762481689453, 0.8966904282569885, 0.9231910109519958, 0.8948338627815247, 0.8552684187889099, 0.9383975863456726, 0.909881055355072, 0.9217467904090881, 0.8874219059944153, 0.9428492188453674, 0.8866745829582214, 0.8421021103858948, 0.8955121636390686, 0.9134024381637573, 0.9135071635246277, 0.895736813545227, 0.8763650059700012, 0.8780710101127625, 0.8612940907478333, 0.8771175146102905, 0.8866831660270691, 0.8938558101654053, 0.8508804440498352, 0.8250914812088013, 0.9106121063232422, 0.9046435952186584, 0.9052439332008362, 0.9043938517570496, 0.8132240176200867, 0.8204460740089417, 0.8905323147773743, 0.931526243686676, 0.8808852434158325, 0.7584546208381653, 0.8885368704795837, 0.85625821352005, 0.7886020541191101, 0.5215569734573364, 0.8664336800575256, 0.8104428648948669, 0.8213629126548767, 0.7762840390205383, 0.8065987229347229, 0.8484461903572083, 0.8272208571434021, 0.8068507313728333, 0.8050483465194702, 0.7918291091918945, 0.9142795205116272, 0.7854698896408081, 0.8907410502433777, 0.7831944823265076, 0.7671591639518738, 0.8811395764350891, 0.9157899022102356, 0.8913277983665466, 0.7729870676994324, 0.9078148603439331, 0.734329104423523, 0.8112693428993225, 0.804324209690094, 0.8331441879272461, 0.8583431839942932, 0.8633409738540649, 0.8204869627952576, 0.8781453967094421, 0.8680794835090637, 0.8045031428337097, 0.883880078792572, 0.8060490489006042, 0.8893559575080872, 0.8178562521934509, 0.8664924502372742, 0.8629710674285889, 0.9038180708885193, 0.8853000998497009, 0.8432589173316956, 0.6902991533279419, 0.8629713654518127, 0.8845977187156677, 0.8180593848228455, 0.8099557757377625, 0.7164834141731262, 0.8002704977989197, 0.8027709126472473, 0.8270023465156555, 0.8162232041358948, 0.8505606651306152, 0.852654755115509, 0.9735432267189026, 0.9533379077911377, 0.8492395281791687, 0.8227349519729614, 0.9082173109054565, 0.8958345055580139, 0.8540036678314209, 0.8856310248374939, 0.9101178050041199, 0.8434793949127197, 0.7845996022224426, 0.6871424317359924, 0.8852116465568542, 0.8561344742774963, 0.8982505798339844, 0.8892454504966736, 0.8857699036598206, 0.8911287188529968, 0.8708028197288513, 0.8898131847381592, 0.7606686353683472, 0.8308326601982117, 0.882247269153595, 0.7362448573112488, 0.8687659502029419, 0.8000958561897278, 0.8208397030830383, 0.8376933932304382, 0.7279537916183472, 0.5314981341362, 0.8569238185882568, 0.8788164258003235, 0.930131733417511, 0.8691627979278564, 0.7748228907585144, 0.9062870144844055, 0.9401468634605408, 0.8485471606254578, 0.8777086138725281, 0.8696633577346802, 0.9200955033302307, 0.8381076455116272, 0.9058613777160645, 0.9199132323265076, 0.9076583385467529, 0.8213265538215637, 0.9632717967033386, 0.9241012930870056, 0.8621293902397156, 0.8581576347351074, 0.8199248909950256, 0.91073077917099, 0.8938822150230408, 0.8890621066093445, 0.6468074917793274, 0.9339860081672668, 0.915277361869812, 0.8045463562011719, 0.8433452248573303, 0.8761388063430786, 0.9275515675544739, 0.8803545832633972, 0.8539025783538818, 0.876520574092865, 0.8691417574882507, 0.9112542271614075, 0.8101365566253662, 0.8455420136451721, 0.8830255270004272, 0.912097156047821, 0.875083327293396, 0.7803565859794617, 0.7985333800315857, 0.8404677510261536, 0.8937333822250366, 0.8293948173522949, 0.9034845232963562, 0.7641649842262268, 0.9183889627456665, 0.8585379719734192, 0.9331988096237183, 0.9027926325798035, 0.9127632975578308, 0.7637465596199036, 0.7804914116859436, 0.8627837300300598, 0.9177753925323486, 0.864728569984436, 0.9218513369560242, 0.871065616607666, 0.9192267060279846, 0.8746844530105591, 0.8757002949714661, 0.9329988956451416, 0.8244927525520325, 0.9303421974182129, 0.9144291281700134, 0.782484769821167, 0.8287388682365417, 0.8721808195114136, 0.8667400479316711, 0.8820070028305054, 0.8307405710220337, 0.8208693861961365, 0.8244592547416687, 0.9079331755638123, 0.8135115504264832, 0.8393158912658691, 0.8883424401283264, 0.9372348189353943, 0.8296979069709778, 0.9109876751899719, 0.8330948948860168, 0.8899182677268982, 0.9179278016090393, 0.874090850353241, 0.9419759511947632, 0.874756395816803, 0.8548253178596497, 0.9136591553688049, 0.8693355917930603, 0.9174781441688538, 0.87677001953125, 0.8558114171028137, 0.8722692131996155, 0.8290491700172424, 0.8752461075782776, 0.866235613822937, 0.8950562477111816, 0.8732865452766418, 0.8681448101997375, 0.8947525024414062, 0.8506854772567749, 0.8828467726707458, 0.7094152569770813, 0.8566680550575256, 0.732505738735199, 0.7926866412162781, 0.8378956317901611, 0.9024633169174194, 0.6450750231742859, 0.9677044749259949, 0.861352264881134, 0.9123423099517822, 0.8996681571006775, 0.9032979607582092, 0.9084280133247375, 0.8595021963119507, 0.9115164875984192, 0.8704807162284851, 0.8832976222038269, 0.8833820223808289, 0.8793954253196716, 0.880059540271759, 0.8987871408462524, 0.9055815935134888, 0.8952341079711914, 0.8779045343399048, 0.9001795053482056, 0.9128628373146057, 0.9457086324691772, 0.9485824108123779, 0.8379718065261841, 0.9048810601234436, 0.9177245497703552, 0.8769530653953552, 0.9231213331222534, 0.7810177803039551, 0.8891920447349548, 0.672785222530365, 0.8759010434150696, 0.7960661053657532, 0.8297738432884216, 0.9084247946739197, 0.8091307878494263, 0.9397733807563782, 0.9269697070121765, 0.9058893918991089, 0.8438166379928589, 0.9052215218544006, 0.8870764374732971, 0.8964157700538635, 0.920748233795166, 0.8517940044403076, 0.820746660232544, 0.9316577315330505, 0.8629191517829895, 0.9334660768508911, 0.8316232562065125, 0.8481443524360657, 0.8126514554023743, 0.809989869594574, 0.8078576326370239, 0.8835054039955139, 0.8521899580955505, 0.7931103110313416, 0.7973893880844116, 0.9250333905220032, 0.9003412127494812, 0.8144167065620422, 0.8140953183174133, 0.821662962436676, 0.7827684879302979, 0.8527292013168335, 0.8938590288162231, 0.9006487131118774, 0.902540385723114, 0.9231579899787903, 0.8764662146568298, 0.9329022169113159, 0.8799895644187927, 0.8987512588500977, 0.8767191767692566, 0.7592930793762207, 0.8962312340736389, 0.8246105313301086, 0.8377309441566467, 0.629403829574585, 0.8225298523902893, 0.911364734172821, 0.7987287640571594, 0.9343813061714172, 0.7340702414512634, 0.8885431885719299, 0.8174503445625305, 0.8866239190101624, 0.9165000915527344, 0.9045320153236389, 0.9328834414482117, 0.7484669089317322, 0.9199981093406677, 0.9194086790084839, 0.8110795617103577, 0.8854605555534363, 0.694142758846283, 0.816865861415863, 0.8645886778831482, 0.9135817289352417, 0.9008502960205078, 0.8733509182929993, 0.8327274918556213, 0.8845672607421875, 0.8305177092552185, 0.901073157787323, 0.8242148756980896, 0.8906667828559875, 0.8995391130447388, 0.938961923122406, 0.8067011833190918, 0.9322828054428101, 0.9166219830513, 0.8948411345481873, 0.9226393699645996, 0.8907405138015747, 0.8507625460624695, 0.9048318862915039, 0.8887925744056702, 0.9030764698982239, 0.8928329348564148, 0.8827155232429504, 0.8670399188995361, 0.8952998518943787, 0.9169678092002869, 0.8283761143684387, 0.9815561771392822, 0.8865774273872375, 0.8775002360343933, 0.975479781627655, 0.8604986071586609, 0.8152527213096619, 0.9696213603019714, 0.8880021572113037, 0.9062009453773499, 0.9112638235092163, 0.8277905583381653, 0.9501254558563232, 0.7988829016685486, 0.770139753818512, 0.8550143837928772, 0.6315186619758606, 0.7274720072746277, 0.916629433631897, 0.6374239325523376, 0.861442506313324, 0.8099598288536072, 0.781500518321991, 0.9236404895782471, 0.842509925365448, 0.7784444689750671, 0.8162652254104614, 0.8296593427658081, 0.8214928507804871, 0.5674732327461243, 0.8632358312606812, 0.8708510994911194, 0.8878651261329651, 0.8008158206939697, 0.7349750399589539, 0.8112398386001587, 0.9173398613929749, 0.7892245650291443, 0.8113029599189758, 0.893355131149292, 0.8936631083488464, 0.9120921492576599, 0.8174864053726196, 0.881058931350708, 0.9468994736671448, 0.9101572632789612, 0.8468867540359497, 0.9117822051048279, 0.8904818296432495, 0.7870612144470215, 0.9200432896614075, 0.870656430721283, 0.7930991053581238, 0.7664735913276672, 0.8270837068557739, 0.9463903307914734, 0.9104788899421692, 0.9561157822608948, 0.8245986104011536, 0.7563440203666687, 0.7388250827789307, 0.9671956896781921, 0.816565215587616, 0.87183678150177, 0.8251832127571106, 0.8978092670440674, 0.8716188073158264, 0.8594432473182678, 0.7732961773872375, 0.9491366744041443, 0.8721122145652771, 0.9087033867835999, 0.8325284123420715, 0.8850669264793396, 0.8977770805358887, 0.7402094006538391, 0.7625457644462585, 0.7331046462059021, 0.8166869282722473, 0.8640335202217102, 0.7664430737495422, 0.9310460686683655, 0.8832929134368896, 0.8732262849807739, 0.8397831916809082, 0.865456759929657, 0.9501627087593079, 0.896897554397583, 0.8309330344200134, 0.892361044883728, 0.8547356128692627, 0.8639182448387146, 0.3991030156612396, 0.6885568499565125, 0.7296175360679626, 0.7958685755729675, 0.8605071306228638, 0.9164314270019531, 0.869161069393158, 0.8627216219902039, 0.8845882415771484, 0.859733521938324, 0.847048282623291, 0.9080760478973389, 0.8781907558441162, 0.909990668296814, 0.8220146894454956, 0.8828198313713074, 0.8578556180000305, 0.8553701639175415, 0.6867508888244629, 0.814727246761322, 0.9223118424415588, 0.91377192735672, 0.8101574778556824, 0.8467799425125122, 0.8592328429222107, 0.8856714367866516, 0.8809605240821838, 0.8352559208869934, 0.8996070027351379, 0.8675628304481506, 0.8500226736068726, 0.8829318284988403, 0.8494887948036194, 0.8598297834396362, 0.827361524105072, 0.851438581943512, 0.7672915458679199, 0.8120284080505371, 0.8507700562477112, 0.7663052678108215, 0.8552660942077637, 0.7394362092018127, 0.8466883301734924, 0.916537880897522, 0.8640434145927429, 0.9157688021659851, 0.8285261988639832, 0.8483935594558716, 0.9273597598075867, 0.8333163857460022, 0.8213348388671875, 0.9383814334869385, 0.9190789461135864, 0.8721311688423157, 0.8786630034446716, 0.7712194919586182, 0.8653947710990906, 0.9022952914237976, 0.8828036189079285, 0.9497464299201965, 0.9315602779388428, 0.884075939655304, 0.9230373501777649, 0.8773588538169861, 0.8439565300941467, 0.9622530341148376, 0.8352480530738831, 0.8554804921150208, 0.8684321045875549, 0.8813677430152893, 0.8194610476493835, 0.8577772974967957, 0.8353118896484375, 0.9276515245437622, 0.940913736820221, 0.8699615597724915, 0.8681623935699463, 0.9482614398002625, 0.9325486421585083, 0.9401525855064392, 0.8577216267585754, 0.7287370562553406, 0.8823885917663574, 0.8679580688476562, 0.7291387915611267, 0.8436459898948669, 0.9481472373008728, 0.8051440119743347, 0.9325991272926331, 0.8688009977340698, 0.9246651530265808, 0.9755653142929077, 0.932216465473175, 0.8839958310127258, 0.9046943783760071, 0.8471404910087585, 0.7963495850563049, 0.9216598868370056, 0.46215513348579407, 0.9020441174507141, 0.8660610318183899, 0.8082026839256287, 0.8223779797554016, 0.84651118516922, 0.9112351536750793, 0.9411863684654236, 0.9453101754188538, 0.9793064594268799, 0.7207394242286682, 0.9097076058387756, 0.9065412282943726, 0.9522144794464111, 0.8553370833396912, 0.7519956231117249, 0.8480797410011292, 0.9696853756904602, 0.8251888155937195, 0.9079187512397766, 0.8560855388641357, 0.8383099436759949, 0.8562954068183899, 0.9483294486999512, 0.8157172203063965, 0.7931143045425415, 0.7764102816581726, 0.9385830760002136, 0.9151163101196289, 0.876132071018219, 0.9235650897026062, 0.8309726715087891, 0.8210564255714417, 0.8873692750930786, 0.8509262204170227, 0.7812409996986389, 0.9361074566841125, 0.9478474259376526, 0.9151849746704102, 0.8732914924621582, 0.8250206708908081, 0.8069981932640076, 0.8941025733947754, 0.8728892207145691, 0.32546466588974, 0.7656130790710449, 0.8457024097442627, 0.9726706147193909, 0.8370704650878906, 0.9494432806968689, 0.8206184506416321, 0.914796769618988, 0.688191831111908, 0.6750509142875671, 0.9005255103111267, 0.9313775897026062, 0.7802155017852783, 0.8864585161209106, 0.9586477875709534, 0.8660703301429749, 0.9012627601623535, 0.8847758173942566, 0.8870525360107422, 0.9289258122444153, 0.9009445905685425, 0.8199372291564941, 0.8900521397590637, 0.9688818454742432, 0.9181717038154602, 0.9212964177131653, 0.8728436827659607, 0.9134387373924255, 0.8696983456611633, 0.8471409678459167, 0.9172108769416809, 0.8391773104667664, 0.8976079821586609, 0.8690103888511658, 0.9091406464576721, 0.6376999616622925, 0.738172709941864, 0.8869271874427795, 0.8196077942848206, 0.7537921071052551, 0.682588517665863, 0.7620252370834351, 0.8912550806999207, 0.8262966275215149, 0.7704908847808838, 0.7543392777442932, 0.798177182674408, 0.7500452399253845, 0.755224883556366, 0.7236307263374329, 0.3466745913028717, 0.7242307066917419, 0.7517543435096741, 0.6480634808540344, 0.798961341381073, 0.8387165069580078, 0.9124752283096313, 0.8831049203872681, 0.9841790795326233, 0.7386912703514099, 0.7641594409942627, 0.9088738560676575, 0.8481200933456421, 0.8875895738601685, 0.6130167841911316, 0.8412668108940125, 0.9464083313941956, 0.7574333548545837, 0.9744380116462708, 0.9001421332359314, 0.8647318482398987, 0.8627276420593262, 0.8873087167739868, 0.9001229405403137, 0.8821791410446167, 0.6079591512680054, 0.9067609906196594, 0.8344677090644836, 0.8565566539764404, 0.8302817344665527, 0.783236026763916, 0.8676325678825378, 0.8321778178215027, 0.864849865436554, 0.8796733021736145, 0.8836119771003723, 0.6210258603096008, 0.7925086617469788, 0.8587850332260132, 0.6753032803535461, 0.758056104183197, 0.8533240556716919, 0.6717837452888489, 0.6664000749588013, 0.7298908233642578, 0.6124088764190674, 0.854826033115387, 0.6857001185417175, 0.7825889587402344, 0.9270910620689392, 0.6614530682563782, 0.8847368955612183, 0.9188566207885742, 0.894281804561615, 0.9563854336738586, 0.7692528963088989, 0.9015615582466125, 0.8338267207145691, 0.8084040880203247, 0.8277961611747742, 0.8424769043922424, 0.8635988831520081, 0.9621680974960327, 0.7780625820159912, 0.6689128279685974, 0.7244668006896973, 0.7324212193489075, 0.43659475445747375, 0.8019682765007019, 0.7861884236335754, 0.8333601355552673, 0.6767374873161316, 0.9575440883636475, 0.8193590044975281, 0.9186696410179138, 0.8303274512290955, 0.7383149266242981, 0.6354231238365173, 0.7465899586677551, 0.5727947354316711, 0.8199698328971863, 0.8545364737510681, 0.7627480626106262, 0.8087603449821472, 0.7238879203796387, 0.6261609792709351, 0.901157557964325, 0.5000264644622803, 0.5859766006469727, 0.6497653126716614, 0.8287231922149658, 0.7767600417137146, 0.7929356694221497, 0.8387042880058289, 0.8310202956199646, 0.9549619555473328, 0.7828732132911682, 0.8267626166343689, 0.9457798004150391, 0.9177148938179016, 0.8941160440444946, 0.9057024717330933, 0.9468358755111694, 0.9356052279472351, 0.8982320427894592, 0.9473466277122498, 0.9370582699775696, 0.9245436191558838, 0.8857330083847046, 0.8798487186431885, 0.8309713006019592, 0.8103129267692566, 0.8087911009788513, 0.9030582308769226, 0.8966541290283203, 0.9030396938323975, 0.8117326498031616, 0.9056372046470642, 0.7411478757858276, 0.8623971343040466, 0.8671626448631287, 0.8114163279533386, 0.8520036339759827, 0.8867087960243225, 0.8342264890670776, 0.8947808146476746, 0.8095459342002869, 0.9247235059738159, 0.8733190894126892, 0.8632487654685974, 0.9345119595527649, 0.8750491142272949, 0.9037219285964966, 0.6529303193092346, 0.9141073226928711, 0.9056088328361511, 0.7986165881156921, 0.9061443209648132, 0.8282671570777893, 0.9009042382240295, 0.9005922675132751, 0.9165424108505249, 0.9418776035308838, 0.9219975471496582, 0.9402656555175781, 0.9230709671974182, 0.886901319026947, 0.8857012391090393, 0.83940190076828, 0.9096437692642212, 0.9107394814491272, 0.8591764569282532, 0.880530059337616, 0.8896896243095398, 0.8830239176750183, 0.7840498685836792, 0.8210979104042053, 0.8154341578483582, 0.6374364495277405, 0.8483254909515381, 0.8058285117149353, 0.8583611249923706, 0.8268886208534241, 0.8944605588912964, 0.5287208557128906, 0.6394777297973633, 0.6983501315116882, 0.7219762206077576, 0.9214989542961121, 0.771126925945282, 0.9236026406288147, 0.9495362043380737, 0.813001275062561, 0.7064937949180603, 0.9137622714042664, 0.7407562732696533, 0.7624629139900208, 0.8859497904777527, 0.5558809638023376, 0.7068899273872375, 0.8776759505271912, 0.6345183849334717, 0.8590968251228333, 0.49384206533432007, 0.9096977114677429, 0.9358625411987305, 0.9120662808418274, 0.8592911958694458, 0.8930125832557678, 0.853310227394104, 0.8592128157615662, 0.785819947719574, 0.9075824618339539, 0.8776280879974365, 0.8600165843963623, 0.9261317849159241, 0.9258683323860168, 0.8730983138084412, 0.8306238055229187, 0.8927620053291321, 0.892832338809967, 0.8617622256278992, 0.9114999175071716, 0.8767852187156677, 0.8962705731391907, 0.9077934622764587, 0.9005783200263977, 0.8617286086082458, 0.9081513285636902, 0.8795148730278015, 0.7655873894691467, 0.9114118218421936, 0.915579080581665, 0.7885870933532715, 0.8641018271446228, 0.8311132788658142, 0.7028757929801941, 0.6643708348274231, 0.8746601939201355, 0.7964195609092712, 0.8476173281669617, 0.928668200969696, 0.8578175902366638, 0.8848134875297546, 0.848082423210144, 0.9416162371635437, 0.8650230765342712, 0.769657552242279, 0.776469349861145, 0.9158303141593933, 0.7867876887321472, 0.7842472195625305, 0.7193610072135925, 0.5397869348526001, 0.9155462384223938, 0.8195365071296692, 0.860417902469635, 0.9054656028747559, 0.8845226168632507, 0.7827541828155518, 0.7725113034248352, 0.808367908000946, 0.7216757535934448, 0.8318071365356445, 0.832615315914154, 0.7329861521720886, 0.8581658601760864, 0.7046560645103455, 0.8473719954490662, 0.7563114166259766, 0.7862815856933594, 0.7248085737228394, 0.7035796046257019, 0.7924213409423828, 0.8587102890014648, 0.796542227268219, 0.9203892350196838, 0.8370152711868286, 0.7222369313240051, 0.6674672961235046, 0.8828400373458862, 0.7799041271209717, 0.8809827566146851, 0.8547495603561401, 0.9484903216362, 0.869442880153656, 0.6189447045326233, 0.8107120394706726, 0.7767320275306702, 0.790334165096283, 0.863534152507782, 0.8768852353096008, 0.8888484239578247, 0.854636549949646, 0.4018678367137909, 0.9470193386077881, 0.9138892292976379, 0.9391964077949524, 0.42123156785964966, 0.8665594458580017, 0.9283613562583923, 0.8658342957496643, 0.9206190705299377, 0.9277269244194031, 0.926257312297821, 0.9443431496620178, 0.8726972937583923, 0.6977158784866333, 0.8648195862770081, 0.9819485545158386, 0.902446448802948, 0.9325900673866272, 0.9524241089820862, 0.7891654372215271, 0.8154374957084656, 0.9236446022987366, 0.9555051326751709, 0.8623347282409668, 0.9012370705604553, 0.38697734475135803, 0.8588841557502747, 0.9172545075416565, 0.9673315286636353, 0.8148449063301086, 0.7114257216453552, 0.9370771646499634, 0.8829830288887024, 0.8387002348899841, 0.9146420359611511, 0.8526241779327393, 0.8034786581993103, 0.9424899816513062, 0.9636416435241699, 0.9173005819320679, 0.8947170972824097, 0.8807384371757507, 0.8021706938743591, 0.8826174736022949, 0.8566595911979675, 0.9263466000556946, 0.8998206257820129, 0.8323125243186951, 0.838033139705658, 0.8577982783317566, 0.8731698393821716, 0.8111688494682312, 0.8955220580101013, 0.8617954254150391, 0.6915245056152344, 0.8092494606971741, 0.8916239142417908, 0.8072157502174377, 0.9579550623893738, 0.8829236626625061, 0.8715288043022156, 0.8632044792175293, 0.9493525624275208, 0.958914577960968, 0.9778024554252625, 0.8725528120994568, 0.9396784901618958, 0.6813220381736755, 0.8150619864463806, 0.9052253365516663, 0.6470348238945007, 0.9024410843849182, 0.7870499491691589, 0.572388768196106, 0.8489757180213928, 0.8933336734771729, 0.6846004724502563, 0.8687860369682312, 0.8138005137443542, 0.7344828248023987, 0.7892574071884155, 0.7910775542259216, 0.8050952553749084, 0.7877352237701416, 0.8516014218330383, 0.8536384105682373, 0.8519782423973083, 0.9172061681747437, 0.8198959231376648, 0.8291237354278564, 0.8391186594963074, 0.8544962406158447, 0.7651795744895935, 0.6895360350608826, 0.8736924529075623, 0.7839892506599426, 0.7676542401313782, 0.897253692150116, 0.6645711064338684, 0.6911442875862122, 0.8085742592811584, 0.8629067540168762, 0.8468217253684998, 0.8245838284492493, 0.840305507183075, 0.866311252117157, 0.9624542593955994, 0.9109463095664978, 0.7714304327964783, 0.8759301900863647, 0.9036089777946472, 0.493326872587204, 0.8321000337600708, 0.8805611729621887, 0.8990438580513, 0.9179176092147827, 0.8363315463066101, 0.8790066838264465, 0.9375973343849182, 0.8807314038276672, 0.6735933423042297, 0.7460460066795349, 0.852864682674408, 0.9224759340286255, 0.9022220969200134, 0.6920522451400757, 0.775363028049469, 0.6384910345077515, 0.813641369342804, 0.8186478614807129, 0.8826459050178528, 0.5595942139625549, 0.9226893782615662, 0.641007661819458, 0.8385923504829407, 0.7829552292823792, 0.5338243842124939, 0.9060315489768982, 0.7161936163902283, 0.8272129893302917, 0.6256681084632874, 0.8073559403419495, 0.877958357334137, 0.8751814365386963, 0.7138129472732544, 0.753515899181366, 0.9324806332588196, 0.8555608987808228, 0.7028421759605408, 0.713661253452301, 0.8484411239624023, 0.8239469528198242, 0.7962666749954224, 0.851479709148407, 0.7215726971626282, 0.9059872031211853, 0.8228274583816528, 0.8535770177841187, 0.9127582311630249, 0.7778225541114807, 0.9127029776573181, 0.7312150597572327, 0.8407816290855408, 0.7774736285209656, 0.8059039115905762, 0.9030515551567078, 0.8973199129104614, 0.7149729132652283, 0.8401928544044495, 0.8105091452598572, 0.8952634334564209, 0.9042805433273315, 0.8594940304756165, 0.8636608719825745, 0.9316056370735168, 0.9021257758140564, 0.9258865118026733, 0.8856160044670105, 0.9242019653320312, 0.882789671421051, 0.8306589722633362, 0.8379965424537659, 0.8702908158302307, 0.8245114684104919, 0.9297643303871155, 0.926577627658844, 0.858277440071106, 0.8825682401657104, 0.9518978595733643, 0.9145107865333557, 0.9068459868431091, 0.871111273765564, 0.8710496425628662, 0.8410878777503967, 0.8927756547927856, 0.845490574836731, 0.9104564189910889, 0.8715372085571289, 0.877070963382721, 0.9285933375358582, 0.8545095324516296, 0.8433113694190979, 0.8463646769523621, 0.8195807337760925, 0.8965379595756531, 0.904990553855896, 0.8272274136543274, 0.8351384997367859, 0.621509850025177, 0.9418174028396606, 0.8130829930305481, 0.875478208065033, 0.8253869414329529, 0.8476683497428894, 0.8948193192481995, 0.8942182660102844, 0.8393281102180481, 0.7811831831932068, 0.880754828453064, 0.8259344696998596, 0.8746453523635864, 0.8501766920089722, 0.9279366731643677, 0.901120126247406, 0.7880677580833435, 0.8797224164009094, 0.8274772763252258, 0.839756429195404], 'system_score': 0.8421334433150697})\n",
            "Avg COMET: 0.8421334433150697\n",
            "COMET (system): 0.842133\n",
            "Saved: /content/drive/MyDrive/mt_eval/qwen3BevalCOMET/gen_de-en_test_20250916-001201_comet.txt\n"
          ]
        }
      ],
      "source": [
        "# === COMET eval: load CSV (src,hyp,ref) ===\n",
        "import os, glob, pandas as pd, torch, time, shutil\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    if not os.path.ismount(\"/content/drive\"):\n",
        "        drive.mount(\"/content/drive\")\n",
        "        print(\"Drive mounted.\")\n",
        "    else:\n",
        "        print(\"Drive already mounted.\")\n",
        "except Exception as e:\n",
        "    print(\"Colab not available:\", e)\n",
        "\n",
        "# 0) Install COMET\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unbabel-comet\"])\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "\n",
        "# 1) Find last generated csv CSV generated or set csv path\n",
        "OUT_DIR = \"/content/drive/MyDrive/mt_eval/qwen3BevalCOMET\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/mt_eval/qwen3BevalCOMET/gen_de-en_test_20250916-001201.csv\" #customize\n",
        "print(\"CSV found:\", CSV_PATH)\n",
        "\n",
        "# 2) Read\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "required = {\"src\",\"hyp\",\"ref\"}\n",
        "missing = required - set(df.columns)\n",
        "assert not missing, f\"Missing columns in csv: {missing}\"\n",
        "data = [{\"src\": s, \"mt\": h, \"ref\": r} for s,h,r in zip(df[\"src\"], df[\"hyp\"], df[\"ref\"])]\n",
        "\n",
        "# 3) Load COMET (ref-based)\n",
        "model_ckpt = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "model = load_from_checkpoint(model_ckpt)\n",
        "\n",
        "# 4) Predict\n",
        "use_gpus = 1 if torch.cuda.is_available() else 0\n",
        "output = model.predict(data, batch_size=64, gpus=use_gpus, progress_bar=True)\n",
        "system_score = float(output.system_score)\n",
        "print(f\"COMET (system): {system_score:.6f}\")\n",
        "# 5) Save COMET score\n",
        "score_path = os.path.splitext(CSV_PATH)[0] + \"_comet.txt\"\n",
        "with open(score_path, \"w\") as f:\n",
        "    f.write(f\"{system_score:.6f}\\n\")\n",
        "\n",
        "print(\"Saved:\", score_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxAvIRZ1WcbM",
        "outputId": "07e68da9-200a-4892-e1e3-a8a53ee4a440"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2006330287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# --- MOUNT DRIVE ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# === CONFIG ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# %% Monta Drive + rimuovi widget dal notebook e salva copia \"clean\" (compatibile GitHub)\n",
        "from google.colab import drive\n",
        "import os, sys, subprocess\n",
        "\n",
        "# --- MOUNT DRIVE ---\n",
        "if not os.path.ismount(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "# === CONFIG ===\n",
        "NB_IN  = \"/content/drive/MyDrive/Colab Notebooks/de-en_qwen_SFT_updated.ipynb\"  # <-- metti qui il tuo file\n",
        "OVERWRITE = False                                              # True per sovrascrivere l'originale\n",
        "NB_OUT = NB_IN if OVERWRITE else NB_IN.replace(\".ipynb\", \"_gh.ipynb\")\n",
        "\n",
        "# --- deps ---\n",
        "try:\n",
        "    import nbformat  # type: ignore\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"nbformat\"])\n",
        "    import nbformat  # type: ignore\n",
        "\n",
        "# --- load ---\n",
        "nb = nbformat.read(NB_IN, as_version=4)\n",
        "\n",
        "# --- strip widget state at notebook level ---\n",
        "for k in [\"widgets\", \"widget_state\", \"application/vnd.jupyter.widget-state+json\"]:\n",
        "    nb.metadata.pop(k, None)\n",
        "if isinstance(nb.metadata.get(\"extensions\"), dict):\n",
        "    nb.metadata[\"extensions\"].pop(\"jupyter_widget_state\", None)\n",
        "\n",
        "# --- strip per-cell widget metadata & outputs ---\n",
        "removed = 0\n",
        "for cell in nb.cells:\n",
        "    if isinstance(getattr(cell, \"metadata\", {}), dict):\n",
        "        for k in [\"widgets\", \"widget_view\", \"widget_state\"]:\n",
        "            cell.metadata.pop(k, None)\n",
        "    outs = []\n",
        "    for out in getattr(cell, \"outputs\", []):\n",
        "        data = getattr(out, \"data\", None)\n",
        "        is_widget = False\n",
        "        if isinstance(data, dict):\n",
        "            wk = [k for k in list(data) if k.startswith(\"application/vnd.jupyter.widget\")]\n",
        "            if wk:\n",
        "                is_widget = True\n",
        "                for k in wk: data.pop(k, None)\n",
        "            if is_widget and not data:\n",
        "                removed += 1\n",
        "                continue\n",
        "        outs.append(out)\n",
        "    cell.outputs = outs\n",
        "\n",
        "# --- save ---\n",
        "nbformat.write(nb, NB_OUT)\n",
        "print(\"Saved:\", NB_OUT)\n",
        "print(\"Widget outputs removed:\", removed)\n",
        "if OVERWRITE:\n",
        "    print(\"ATTENZIONE: file originale sovrascritto.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri0t-AQhXAEI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyPCKU+ojv86XEeUNh3lhk5A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}