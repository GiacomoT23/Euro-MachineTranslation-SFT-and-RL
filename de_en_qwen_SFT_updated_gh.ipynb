{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiacomoT23/Euro-MachineTranslation-SFT-and-RL/blob/main/de_en_qwen_SFT_updated_gh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBJQPZzyHr99"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzQq9SE-Hlvz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-input --upgrade pip\n",
        "!pip install --no-input unsloth bitsandbytes accelerate peft trl sentencepiece protobuf hf_transfer\n",
        "!pip install --no-input transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq3XZU80Hu3M"
      },
      "source": [
        "# Model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499,
          "referenced_widgets": [
            "3d5d3cee87324b609dd34caeec290b80",
            "aafcd063243945c986ca32e067c15b17",
            "f173c793035e4f80899f19c99826f4c2",
            "7fa79fbef0624a6e84f927372a6e6407",
            "01a6f11ac01c4ec8bc78ee9f7a5e5131",
            "3bb70b2fe8d34e1e8e3ec1a2adda3230",
            "fde44b2023494dc0b29fc6f2edff5d27",
            "8a2b1d5edf2c4b33b1eca4077d7db70a",
            "8831889406ec437394f07650190cb1a5",
            "e6914c61d2eb43fe80f2ff6f50a629a0",
            "ca11794c6f924599a2b5e73e15cb25aa",
            "c6afe263ca3843f2a89febbe148bf06a",
            "661b4b7f22ec4b809a2de346f4c72183",
            "5365021ebcdd4fb38df80f1e249990ff",
            "4c5dc91e6494467b9d53c5ba834e6274",
            "2a5a4eea9d4c433aac6dbd8ac6ca7ea0",
            "e48396dbf83c4c409795895f393e0b0f",
            "2662a04c687d451082ccffa069749c82",
            "d99d5e051c974cff902a9c688ea67e28",
            "be35e1b3a411475da8be8dc4922e053d",
            "d29ab5e19de34eeea2368f726c6bdd5a",
            "426ebebbbadc4d6f9e63e617c951da79",
            "cc93aa3ea5c84a25ba6f229ca624273f",
            "92463b5429f4488d8faf59636cde047a",
            "490cbed1975a48c9bb9888b1156fdbff",
            "c86c6206d90f421d9606fb7dd3efb050",
            "e63ba00d9c2545338fb092ef144ba42c",
            "45af0b00eb5f4bfd90ec0dfa759fdc6d",
            "238d0e2c353c423fb8714066844f86aa",
            "318971ded71e4922b61cb10c3eba21d6",
            "237361c93205422e99c4a43a658f34cd",
            "626dae3cf9eb424c8405ee76ad065447",
            "2ae2d9eeb3ed4fa4bc3e092cd46891d4",
            "91b0a1d6e6544d50a44bbd72d311288b",
            "98e8d3acce7149599d8a742786283973",
            "4eb2399a45cf46228463e07ee9f6c4e2",
            "cdda04f0b4fd4435adb349858426e1b8",
            "ccced2be6ed24147a92c5be0d1ee1027",
            "8c80e73d649544969929c78f0c6f4cf1",
            "9dbae2a96b4244bba6165f17ec81f8fc",
            "28a1c809e8244ea2b237f4d75d6e4484",
            "51ce15a8f29c41d397c8d5586c3ec99f",
            "506ccb4439aa4570bcfe68fc295f4d1a",
            "6d264fe9c84b4b9494502fc2ce9495bc",
            "36ee64f402cb48328fd5530ade545edc",
            "3a702e1eabd744c2952aa85ec77d017c",
            "50c64765424f41fc85d4764ce026327d",
            "868c522f4429404eb83f6063b12dd7e3",
            "ebf238cde7f84551af979f0aa0b6dcce",
            "cdf87219a864431da380fa74c059f3c9",
            "0f431b38387c454e986550158c7d6058",
            "5bfc6a377f0840be985c998dcc7a607f",
            "455a84d1657a41a6bbcf367a9fae0d88",
            "f2736f2d1c884d17a42759112fe00b31",
            "619a86b3390f4488849c8280419db612",
            "c241861ca6394a9fb354d705159675f2",
            "4e2531c3e5d54e868ed6d380c8bdf590",
            "8a8c75bce621437b83f0b3a2b591c629",
            "5cf497f3c9544607a460f1e069b70dc4",
            "40ccc64e6e9b401eb65a09a0784a288a",
            "5e8c8706d91c42da8045bd6b1a03280e",
            "6a969e4ca6814ece97fb5d98a606a68c",
            "2ec6a747468a475cad9d08219f6a1247",
            "fa40fcd37d8b48e7830aa6f9de24ee3d",
            "1920154a09c1496fa7db73abca1459ae",
            "5334e18a99c741e09d886db7b7f0b747",
            "8ac4491866124870b3b1ae10e5e7ec00",
            "4599028308104fcd9e45c01b8d72a508",
            "57cf5660d00945a18b07128813c28474",
            "9c89996444e94becb36dd22e20aeddbb",
            "392da9d726ef423f93edf9779f22d817",
            "c23a9d45569b4aeabc19f8052694ef81",
            "53afe0d8abc241379189ddfc268657e8",
            "a81fb8f4c3f64b23afb8ce660a701cd0",
            "77e1a23c1864422782ee1e3baea1885f",
            "37cb10fb0b614897952dc54fd7e38e73",
            "1a441811d3514919b855d0cf8aacc113",
            "a4c4d1b365334786b5bbe96e53a9fec7",
            "d2bc7bd5c5ec4e59898956303fa5515d",
            "978df4d8cb8645daac3ef033ef268de8",
            "32f0fad526c04d2fa3aa564098af9d62",
            "31b228ec52a24cad8071903d4b22268d",
            "cc1938de17d64011834d1acc0fac84a4",
            "e8cc425193804c2084b89534e833ea04",
            "1530b4b4c5914a0c9fa2df5d3e3b3de1",
            "3e8bc4b90515477e823998211654c216",
            "1316a3b4c62947169204bb8e7506c046",
            "83e1d4a9f44441acb30174cd23b92896"
          ]
        },
        "id": "14YjMw2QHvTR",
        "outputId": "1d4b06ab-8156-46ce-8ae3-84ef079691f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.4: Fast Qwen2 patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2025.9.4 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: unsloth/Qwen2.5-3B | 4bit=True | GC=unsloth | seq=256\n",
            "Trainable params (LoRA+heads): ~29,933,568\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch, os\n",
        "\n",
        "USE_4BIT = True\n",
        "USE_GC   = \"unsloth\"\n",
        "MAX_SEQ  = 256\n",
        "MODEL_ID = \"unsloth/Qwen2.5-3B\"\n",
        "\n",
        "dtype = None if USE_4BIT else torch.float16\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = MODEL_ID,\n",
        "    max_seq_length = MAX_SEQ,\n",
        "    dtype          = dtype,\n",
        "    load_in_4bit   = USE_4BIT,\n",
        ")\n",
        "\n",
        "# LoRA\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = USE_GC,\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model: {MODEL_ID} | 4bit={USE_4BIT} | GC={USE_GC} | seq={MAX_SEQ}\")\n",
        "print(f\"Trainable params (LoRA+heads): ~{n_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlXY57VDIEde"
      },
      "source": [
        "# Loading splits\n",
        "Loading the preprocessed dataset saved on drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iVVPuajIE2t",
        "outputId": "472b4edc-d12a-407a-def7-64a33f88f234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "{'train': 111966, 'validation': 2780, 'test': 3003}\n"
          ]
        }
      ],
      "source": [
        "# === Load filtered dataset from Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from datasets import load_from_disk, DatasetDict\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/mt_datasets/wmt14_de-en_sample150000__filtered__len169_lid95_r2.25_ck60_trainVAL_filtered_testRAW\"\n",
        "ds_proc = load_from_disk(SAVE_DIR)\n",
        "\n",
        "train_filt = ds_proc.get(\"train\")\n",
        "val_filt   = ds_proc.get(\"validation\")\n",
        "test_filt  = ds_proc.get(\"test\")\n",
        "\n",
        "print({k: len(v) for k, v in ds_proc.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1f0qX5UIa68"
      },
      "source": [
        "# Formatting for SFT in unsloth\n",
        "Formatting with system-user-assistant template. Model will be trained only on assistant responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556,
          "referenced_widgets": [
            "2cf85bbb94624224b3703a08b646f575",
            "f5a4a250d3474b17b2e83eb6770d0a52",
            "c6d0def0289d42229d16bee9e6321d83",
            "e889e658cf974b49b969eead59386c69",
            "1d74ae98edcf4aaca3693a63547d681d",
            "699bcd36dd07405d99d55cb43f02dfe6",
            "f5abb8e9a8ce40f2bbc0c6c3514a4523",
            "8309e35ef9db4d84ba5c5033e390dab3",
            "3ae09064e105431ca6bf05567a3c9bd0",
            "a100975a57534d6f868bfb83f4a16288",
            "567b455fe0924b01b29d8bcb7f02c417",
            "f9675c6887f4434d8c7fed9b3c8c3709",
            "1bd54889aa06464d822eb1cb809e5fca",
            "deb2bff8a7e8492c9577f999ee0b66e3",
            "426110b090344682afccf48f337a6af8",
            "312fccb0ff9841698b8ff28b604911dc",
            "8d53b45c47b14279bd5aa9bbefdeb8d1",
            "a786e2c89b6646dca38d72761b9440a4",
            "333b8eb38d7e4d4dacbd1a0d69df3fe7",
            "a26069a0a4d445a78f43de5cd88585a9",
            "cca7bbea00f8424c9601aaac6fd90dab",
            "70ccd0f9e7b04cca8f91a949abd310bc",
            "cce363aaa7b94f3fac0ad76870def492",
            "3f7a28e6d45645f28a29ddbe50ad4847",
            "d74954f965874f7689541e1659522e5f",
            "b97df726d6754f8eb915893feac17c98",
            "97e91553ca294e208454146ba36829b0",
            "23ee1a0095ee40e1bc99e533754ffcbb",
            "eb0c829fefa74bdebabffbc033c4d092",
            "71758a4d4adf4253a61f0dc21274b57d",
            "d61a8bc26260410da9c7020bbfb9d74c",
            "2fb8e4b14a7c434785bb6932116882fc",
            "cc370250c8af446a9db8ce30512a7701",
            "42d4bddda1c14f6dba40bbd2951bbc32",
            "aec474ebb34d42939d065a1c1ff25c8b",
            "fe762c220b374026a960a200735de4cd",
            "c01ec511848c4cffaca9934d8c29c113",
            "f252ed0889b44150888ad5d2d2014d22",
            "3aee192c2ba84162adc683e7be3ccea1",
            "7725bad7fdb849e889435aae12d5cb2e",
            "b5de91e2443b4d4fad5e6a33286cccb1",
            "895b0abb0ec1498cafeb1e41c951b60e",
            "49619eb5c357437984a5071f4730c710",
            "7273b13ad4824a5cbfbeed492c6241de",
            "ff384df03fbb442aacc7123e043cf55f",
            "5f55dec44b69450fb31864235d4319f4",
            "3c90b9ab1bd843f9a4d61e5a7c1b65a2",
            "10843e0712874c908381549193c7ad8a",
            "685dd73ea0724c109013b5108c2671df",
            "aaa0539fe1bf41afb12fab0c41f724f3",
            "e135b6de0e5042ba9ec3cd542ca58fb7",
            "320e212f8ea8497f8ef2d8051971ca54",
            "843350ff829f4cc19d0b3861796f49e1",
            "dc1e3a78144a494aa1cb27a994bcb21a",
            "a03545fcad7d40e38470e677eb42b57a",
            "76b089880d9b4712b824f151cf2db938",
            "af2ef3237431426ea3b27ed7609292fb",
            "adc5ec8a59154a34ac184522cf09f1bd",
            "80bacf2c590e4a82b79232b376b0e338",
            "96505f82c0724c74989060c3715f2a3f",
            "fcbc9be8c1e047c48ee72d348dec55f8",
            "0cfc7c90b7e941a8b9b319b8f26d8b8b",
            "7999976722804132973801fcefe44d6a",
            "b3921d0da03647269864fad8264c1ee9",
            "b1978166aa384d01af83ca9d10cdc76d",
            "cace8bcae23f4a2ba3c2dbc1633a35ed",
            "b974f030673048c0a6ef5a15e533b302",
            "75d9a8c53eda42bb8ac079d2428b733b",
            "f7ecd213b5a84d85958c1f2c14002790",
            "782122865f1746a3935328574a2f86fd",
            "f620ce5ded3d477bac75e305654f956a",
            "41cf193886554b9f8e14b5b29112951e",
            "0a3f715b2ae04692b177095b2a9855ee",
            "fb871394780f4c029d16aec1ce6c7fa0",
            "4c02a32536bf442aa7a66c3ce07047fb",
            "91c33ecbc6cb403f8b6169d967d3a7f8",
            "419be183f9684971bfe73ebbac1efbdc"
          ]
        },
        "id": "0PYIqJDTIbMi",
        "outputId": "bd17e5fd-120a-4b1b-86f7-664621f6f52a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[normalize] add src_txt/tgt_txt from translation:   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[normalize] add src_txt/tgt_txt from translation:   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[normalize] add src_txt/tgt_txt from translation:   0%|          | 0/3003 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[chatml] make text (train):   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[chatml] make text (val):   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val sizes: 111966 2780\n",
            "Esempio text:\n",
            " <|im_start|>system\n",
            "You are a translation engine. Translate from German (de) to English (en).<|im_end|>\n",
            "<|im_start|>user\n",
            "Dabei handelt es sich um das Berliner Ãœbereinkommen von 1937 und das StraÃŸburger Ãœbereinkommen von 1973, die beide recht alt, der heutigen Sachlage in Europa nicht mehr angemessen und demnach weitgehend Ã¼berholt sind.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "These are the 1937 Berlin agreement and the 1973 Strasbourg agreement which, because they are quite old, are not relevant to the situation we have in Europe today and have therefore become largely obsolete.<|im_end|>\n",
            "\n",
            "Esempio text:\n",
            " <|im_start|>system\n",
            "You are a translation engine. Translate from German (de) to English (en).<|im_end|>\n",
            "<|im_start|>user\n",
            "Dieser Mix fÃ¼hrte wieder zu lebhaften GesprÃ¤chen unter den Teilnehmern in den Pausen und wÃ¤hrend der Abendveranstaltung, die von allen Beteiligten an dieser Veranstaltung in den Gesamtbeurteilungen als sehr informativ empfunden wurde.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "This mixture, once again, facilitated lively discussions among the participants during the breaks and, of course, during the evening event and was found to be very informative as expressed in the overall feedback survey.<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "import unicodedata\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
        "\n",
        "SRC, TGT = \"de\", \"en\"\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize(\"NFKC\", (s or \"\"))\n",
        "\n",
        "def ensure_src_tgt_cols(dset):\n",
        "    cols = set(dset.column_names)\n",
        "    if {\"src_txt\",\"tgt_txt\"}.issubset(cols):\n",
        "        return dset\n",
        "    elif \"translation\" in cols:\n",
        "        def to_cols(batch):\n",
        "            src = [nfkc(x.get(SRC, \"\")) for x in batch[\"translation\"]]\n",
        "            tgt = [nfkc(x.get(TGT, \"\")) for x in batch[\"translation\"]]\n",
        "            return {\"src_txt\": src, \"tgt_txt\": tgt}\n",
        "        return dset.map(\n",
        "            to_cols, batched=True, batch_size=4096,\n",
        "            desc=\"[normalize] add src_txt/tgt_txt from translation\",\n",
        "        )\n",
        "    else:\n",
        "        raise KeyError(\"Split has not src_txt/tgt_txt or translation.\")\n",
        "\n",
        "train_norm = ensure_src_tgt_cols(train_filt)\n",
        "val_norm   = ensure_src_tgt_cols(val_filt) if val_filt is not None else None\n",
        "test_norm  = ensure_src_tgt_cols(test_filt) if test_filt is not None else None\n",
        "\n",
        "LANG_NAME = {\"de\": \"German\", \"en\": \"English\"}\n",
        "def lang_name(code: str) -> str: return LANG_NAME.get(code.lower(), code.upper())\n",
        "\n",
        "SYSTEM_TMPL = \"You are a translation engine. Translate from {src_name} ({src_code}) to {tgt_name} ({tgt_code}).\"\n",
        "\n",
        "def to_text_chatml(batch, src_code=SRC, tgt_code=TGT):\n",
        "    srcs, tgts = batch[\"src_txt\"], batch[\"tgt_txt\"]\n",
        "    texts = []\n",
        "    sys_msg = SYSTEM_TMPL.format(\n",
        "        src_name=lang_name(src_code), tgt_name=lang_name(tgt_code),\n",
        "        src_code=src_code, tgt_code=tgt_code,\n",
        "    )\n",
        "    for s, t in zip(srcs, tgts):\n",
        "        s1, t1 = (s or \"\").strip(), (t or \"\").strip()\n",
        "        if not s1 or not t1:\n",
        "            texts.append(\"\"); continue\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": sys_msg},\n",
        "            {\"role\": \"user\",   \"content\": s1},\n",
        "            {\"role\": \"assistant\", \"content\": t1},\n",
        "        ]\n",
        "        txt = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(txt)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "train_ds = train_norm.map(\n",
        "    to_text_chatml, batched=True,\n",
        "    remove_columns=train_norm.column_names,\n",
        "    desc=\"[chatml] make text (train)\",\n",
        ").filter(lambda ex: len(ex[\"text\"]) > 0)\n",
        "\n",
        "val_ds = None\n",
        "if val_norm is not None:\n",
        "    val_ds = val_norm.map(\n",
        "        to_text_chatml, batched=True,\n",
        "        remove_columns=val_norm.column_names,\n",
        "        desc=\"[chatml] make text (val)\",\n",
        "    ).filter(lambda ex: len(ex[\"text\"]) > 0)\n",
        "\n",
        "print(\"Train/Val sizes:\", len(train_ds), (0 if val_ds is None else len(val_ds)))\n",
        "print(\"Esempio text:\\n\", train_ds[0][\"text\"])\n",
        "print(\"Esempio text:\\n\", train_ds[1][\"text\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8aiRYQHJ2Et"
      },
      "source": [
        "# SFT\n",
        "SFT parameters are set here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "81c6d78d33204b59a6b4224cc1d8f6b7",
            "cf9b4099482745e1afe6c09a5598edb8",
            "a6c9c3697cc845d2af6f4337fd9a2b09",
            "15ed5baff48847b9a972fbc016bf76e3",
            "462e96dab1be49bc95d37e6eb4626b6a",
            "8a83881518904489b51dbfa49110970f",
            "674386b729ce4a9ea652e425210bff79",
            "1fa941d979194db3a07e2e8781274285",
            "32e9db79e2fc4d54a09887811f2f80c0",
            "9ab5aa84c4f045a6badcd51324c36929",
            "d9bdaefbabb7407787d62da651588db1",
            "b5803dc552044f3d974a6d6d0eefe3be",
            "b1e14dc6c7994f29935f9e3d02c89cf5",
            "2e9f47e29647446b9c136827d512c949",
            "cf900f12523f4d62ac3909b29a4130f2",
            "b0b391d397404a50b3e6cd59cf9552a9",
            "58069a2120c147a4a048f0ce1b50c775",
            "78090fe25e6449dabb7e586c4b08f10b",
            "5c9bb5d4d61e4290ba51cb7979253825",
            "8374d25c76884c2d841ffa6a30a1c5af",
            "828dde1773f24d2582984461e6f00659",
            "700b2b5b2c094daf900d0fd99106a049",
            "a1baaf5d22d1422b8b25a2ebad1ed186",
            "294419fbe5d7481392c0a73897e04814",
            "b171a434e7a74fd18b0782ee58ce609a",
            "cdc64bf26ec441d4903eb911c7db88c5",
            "93bb2645a2f74d04b7efc93b8bc47dd0",
            "15d15c7bd5514e009b9a941cf7713fea",
            "d5aa9366a3df44ecae420ec6cb24e055",
            "5d744ccc4d0345578e49c351eaa9bb31",
            "b87e7341c4a64ef586deb25f9b8f14df",
            "af1d25fadd8b4d63b9c4a330918167dd",
            "b86e0ac0c49a4763bf9dc66768353cb1",
            "8343d3ecf00745e88679272810023729",
            "e21726cc9ee041e88494146dff805522",
            "a2289627b80d446f8e3b83fc65bb49bf",
            "6574f494228e41e4acdc3a789d4073de",
            "c69a3f4f526c4383927bafcc60f2f6e0",
            "4c3e727644c040eda60e68945235c372",
            "9ed33cda9cb142a498814c740e413b81",
            "e341aebf8ae64cf0aa19b40b407542ce",
            "818280f492b14409aab57667239a2842",
            "fc769125718347259c0cf4d7104a13f4",
            "58fdc5ee0c634efb9e870c6ced6c3e0d"
          ]
        },
        "id": "e6CbidiOJvNa",
        "outputId": "79ad726a-b806-4cac-dd84-9ac90f650077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Map (num_proc=12):   0%|          | 0/111966 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Map (num_proc=12):   0%|          | 0/2780 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val sizes: 111966 2780\n",
            "Precision: bf16\n",
            "Ready. Training on assistant spans only (Qwen 2.5 ChatML).\n"
          ]
        }
      ],
      "source": [
        "# === SFT with TRL + W&B (Qwen2.5-3B, QLoRA) â€” ChatML + train_on_responses_only ===\n",
        "import os, time, wandb, torch\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import EarlyStoppingCallback\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "# pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# W&B login\n",
        "os.environ[\"WANDB_PROJECT\"]   = \"euromt\"\n",
        "os.environ[\"WANDB_NAME\"]      = f\"qwen25-3b-sft_chatml_{len(train_ds)}_{int(time.time())}\"\n",
        "os.environ[\"WANDB_WATCH\"]     = \"false\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
        "os.environ[\"WANDB_SILENT\"]    = \"true\"\n",
        "if not os.getenv(\"WANDB_API_KEY\"):\n",
        "    from getpass import getpass\n",
        "    os.environ[\"WANDB_API_KEY\"] = getpass(\"W&B API key: \")\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"euromt\",\n",
        "    name=\"qwen25-3b-wmt14-de-en-sft_chatml\",\n",
        "    config={\"model\": MODEL_ID, \"max_seq_len\": 256, \"use_4bit\": True, \"lora_r\": 16, \"train_rows\": len(train_ds)},\n",
        "    settings=wandb.Settings(start_method=\"thread\"),\n",
        ")\n",
        "\n",
        "MAX_SEQ    = 256\n",
        "PER_DEV_BS = 128\n",
        "GRAD_ACCUM = 1\n",
        "\n",
        "use_gpu  = torch.cuda.is_available()\n",
        "name     = torch.cuda.get_device_name(0).lower() if use_gpu else \"\"\n",
        "is_a100  = \"a100\" in name\n",
        "bf16_flag = bool(is_a100)\n",
        "fp16_flag = bool(use_gpu and not is_a100)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_length=MAX_SEQ,\n",
        "    packing=False,\n",
        "    args=SFTConfig(\n",
        "        output_dir=\"outputs_sft\",\n",
        "        report_to=[\"wandb\"],\n",
        "\n",
        "        per_device_train_batch_size=PER_DEV_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        num_train_epochs=2,\n",
        "        max_steps=-1,\n",
        "\n",
        "        learning_rate=1e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.05,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=0.6,\n",
        "        optim=\"adamw_8bit\",\n",
        "\n",
        "        #group_by_length=True,\n",
        "        dataloader_num_workers=2,\n",
        "        dataloader_pin_memory=True,\n",
        "\n",
        "        logging_steps=20,\n",
        "        eval_strategy=\"steps\" if val_ds is not None else \"no\",\n",
        "        eval_steps=150,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=150,\n",
        "        save_total_limit=3,\n",
        "\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        load_best_model_at_end=True,\n",
        "\n",
        "        seed=3407,\n",
        "        bf16=bf16_flag,\n",
        "        fp16=fp16_flag,\n",
        "    ),\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0)],\n",
        ")\n",
        "\n",
        "# Loss only on responses\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|im_start|>system\\n\",\n",
        "    response_part    = \"<|im_start|>assistant\\n\",\n",
        ")\n",
        "\n",
        "print(\"Train/Val sizes:\", len(train_ds), (0 if val_ds is None else len(val_ds)))\n",
        "print(\"Precision:\", \"bf16\" if bf16_flag else (\"fp16\" if fp16_flag else \"fp32\"))\n",
        "print(\"Ready. Training on assistant spans only (Qwen 2.5 ChatML).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "OmS4bWUWebUN",
        "outputId": "987f47e8-def3-4e64-87a5-015747b0b273"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 111,966 | Num Epochs = 2 | Total steps = 1,750\n",
            "O^O/ \\_/ \\    Batch size per device = 128 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (128 x 1 x 1) = 128\n",
            " \"-____-\"     Trainable parameters = 29,933,568 of 3,115,872,256 (0.96% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1400' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1400/1750 1:46:31 < 26:40, 0.22 it/s, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.415100</td>\n",
              "      <td>1.455451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.356200</td>\n",
              "      <td>1.442189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.357200</td>\n",
              "      <td>1.437055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.352200</td>\n",
              "      <td>1.428804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.322900</td>\n",
              "      <td>1.436336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.315700</td>\n",
              "      <td>1.435536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.299000</td>\n",
              "      <td>1.433643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1400, training_loss=1.3641400527954102, metrics={'train_runtime': 6402.4467, 'train_samples_per_second': 34.976, 'train_steps_per_second': 0.273, 'total_flos': 5.810809133829489e+17, 'train_loss': 1.3641400527954102, 'epoch': 1.6})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resume = False\n",
        "trainer.train(resume_from_checkpoint=resume)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXFNFuZgZrwd"
      },
      "source": [
        "# Save checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3DohN09R2IX",
        "outputId": "c23760e9-92dc-4edd-f44b-59226f725d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Copied BEST checkpoint in:\n",
            "/content/drive/MyDrive/mt_checkpoints/qwen25_sft_best_20250914_235740\n"
          ]
        }
      ],
      "source": [
        "# === Save on Google Drive the BEST (or LAST) checkpoint ===\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os, shutil, glob, time\n",
        "\n",
        "OUT_DIR = \"outputs_sft\"  # SFTConfig.output_dir\n",
        "\n",
        "def pick_checkpoint(out_dir):\n",
        "    best = getattr(trainer.state, \"best_model_checkpoint\", None)\n",
        "    if best and os.path.isdir(best):\n",
        "        return best, \"best\"\n",
        "    ckpts = sorted(glob.glob(os.path.join(out_dir, \"checkpoint-*\")), key=os.path.getmtime)\n",
        "    if ckpts:\n",
        "        return ckpts[-1], \"last\"\n",
        "    return None, None\n",
        "\n",
        "ckpt_path, kind = pick_checkpoint(OUT_DIR)\n",
        "assert ckpt_path is not None, f\"No checkpoint found in: {OUT_DIR}\"\n",
        "\n",
        "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "GDRIVE_DIR = f\"/content/drive/MyDrive/mt_checkpoints/qwen25_sft_{kind}_{stamp}\"\n",
        "\n",
        "shutil.copytree(ckpt_path, GDRIVE_DIR)\n",
        "tokenizer.save_pretrained(GDRIVE_DIR)  # save also tokenizer\n",
        "\n",
        "print(f\"Copied {kind.upper()} checkpoint in:\\n{GDRIVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7BXhq_XZuEA"
      },
      "source": [
        "# Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8xeqmsoZv_N"
      },
      "outputs": [],
      "source": [
        "# === LOAD best checkpoint (LoRA) + tokenizer + dataset from Drive ===\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os, glob, unicodedata, torch\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "from datasets import load_from_disk, DatasetDict\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL_ID   = \"unsloth/Qwen2.5-3B\"\n",
        "MAX_SEQ    = 256\n",
        "USE_4BIT   = True\n",
        "SRC, TGT   = \"de\", \"en\"\n",
        "\n",
        "# checkpoint da Drive (quello salvato nella cella 1)\n",
        "CKPT_DIR   = \"/content/drive/MyDrive/mt_checkpoints/qwen25_sft_best_YYYYMMDD_HHMMSS\"\n",
        "print(\"Checkpoint:\", CKPT_DIR)\n",
        "\n",
        "# Dataset\n",
        "DATASET_DIR = \"/content/drive/MyDrive/mt_datasets/wmt14_de-en_proc\"\n",
        "\n",
        "# --- Load base model + tokenizer ---\n",
        "dtype = None if USE_4BIT else torch.float16\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = MODEL_ID,\n",
        "    max_seq_length = MAX_SEQ,\n",
        "    dtype          = dtype,\n",
        "    load_in_4bit   = USE_4BIT,\n",
        ")\n",
        "# Apply chat template\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "base_model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "base_model.config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# --- Attach Lora adapters ---\n",
        "model = PeftModel.from_pretrained(base_model, CKPT_DIR)\n",
        "model.eval()\n",
        "\n",
        "# --- Load standardized dataset ---\n",
        "def nfkc(s: str) -> str: return unicodedata.normalize(\"NFKC\", (s or \"\").strip())\n",
        "\n",
        "def project_to_src_tgt(dset, src_code=SRC, tgt_code=TGT):\n",
        "    cols = set(dset.column_names)\n",
        "    if {\"src_txt\",\"tgt_txt\"}.issubset(cols):\n",
        "        return dset\n",
        "    elif \"translation\" in cols:\n",
        "        def to_cols(batch):\n",
        "            src = [nfkc(ex.get(src_code, \"\")) for ex in batch[\"translation\"]]\n",
        "            tgt = [nfkc(ex.get(tgt_code, \"\")) for ex in batch[\"translation\"]]\n",
        "            return {\"src_txt\": src, \"tgt_txt\": tgt}\n",
        "        return dset.map(to_cols, batched=True, desc=\"Project translation â†’ src/tgt\")\n",
        "    else:\n",
        "        raise ValueError(f\"Split schema non riconosciuto: {cols}\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "ds_eval = DatasetDict()\n",
        "if os.path.isdir(DATASET_DIR):\n",
        "    ds_proc = load_from_disk(DATASET_DIR)\n",
        "    for sp in (\"train\",\"validation\",\"test\"):\n",
        "        if sp in ds_proc: ds_eval[sp] = project_to_src_tgt(ds_proc[sp])\n",
        "else:\n",
        "    raw = load_dataset(\"wmt14\", \"de-en\")\n",
        "    ds_eval[\"test\"] = project_to_src_tgt(raw[\"test\"])\n",
        "\n",
        "print(\"âœ“ Splits:\", {k: len(v) for k, v in ds_eval.items()})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2dCihQtZyFp"
      },
      "source": [
        "# Evaluate with **comet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELtD4G93Z2m4"
      },
      "outputs": [],
      "source": [
        "# === Fast GENERATION + COMET evaluation on TEST (ChatML-consistent) ===\n",
        "!pip -q install \"unbabel-comet>=2.2.4\"\n",
        "\n",
        "import os, time, numpy as np, pandas as pd, torch\n",
        "from tqdm.auto import tqdm\n",
        "from comet import download_model, load_from_checkpoint\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "FastLanguageModel.for_inference(model)\n",
        "model.eval()\n",
        "\n",
        "# ----------------------- Config -----------------------\n",
        "SPLIT               = \"test\"\n",
        "MAX_EVAL_EXAMPLES   = None\n",
        "GEN_BATCH_SIZE      = 128\n",
        "MAX_NEW_TOKENS      = 128\n",
        "DO_SAMPLE           = False\n",
        "COMET_BATCH_SIZE    = 128\n",
        "BASE_DIR            = \"/content/drive/MyDrive/mt_eval\"\n",
        "\n",
        "SRC, TGT = \"de\", \"en\"\n",
        "LANG_NAME = {\"de\": \"German\", \"en\": \"English\"}\n",
        "def lang_name(code: str) -> str: return LANG_NAME.get(code.lower(), code.upper())\n",
        "\n",
        "SYSTEM_TMPL = \"You are a translation engine. Translate from {src_name} ({src_code}) to {tgt_name} ({tgt_code}).\"\n",
        "\n",
        "assert SPLIT in ds_eval and {\"src_txt\",\"tgt_txt\"}.issubset(ds_eval[SPLIT].column_names)\n",
        "dset = ds_eval[SPLIT]\n",
        "n_all = len(dset)\n",
        "take = n_all if MAX_EVAL_EXAMPLES is None else min(MAX_EVAL_EXAMPLES, n_all)\n",
        "subset = dset.select(range(take))\n",
        "sources = list(subset[\"src_txt\"])\n",
        "refs    = list(subset[\"tgt_txt\"])\n",
        "print(f\"Evaluating {len(sources)}/{n_all} examples on '{SPLIT}'\")\n",
        "\n",
        "# ----------------------- Generation (ChatML) -----------------------\n",
        "@torch.inference_mode()\n",
        "def generate_batch_chatml(src_batch, src_code=SRC, tgt_code=TGT):\n",
        "    sys_msg = SYSTEM_TMPL.format(\n",
        "        src_name=lang_name(src_code), tgt_name=lang_name(tgt_code),\n",
        "        src_code=src_code, tgt_code=tgt_code,\n",
        "    )\n",
        "    messages_list = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": sys_msg},\n",
        "            {\"role\": \"user\",   \"content\": s.strip()},\n",
        "        ]\n",
        "        for s in src_batch\n",
        "    ]\n",
        "\n",
        "    # Left padding for generation\n",
        "    prev_pad, prev_trunc = tokenizer.padding_side, getattr(tokenizer, \"truncation_side\", \"right\")\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "\n",
        "    try:\n",
        "        enc = tokenizer.apply_chat_template(\n",
        "            messages_list,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=DO_SAMPLE,\n",
        "            use_cache=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    finally:\n",
        "        tokenizer.padding_side = prev_pad\n",
        "        tokenizer.truncation_side = prev_trunc\n",
        "\n",
        "    # Decode only new tokens\n",
        "    cut = enc[\"input_ids\"].size(1)\n",
        "    hyps = [tokenizer.decode(outputs[i, cut:], skip_special_tokens=True).strip()\n",
        "            for i in range(outputs.size(0))]\n",
        "    return hyps\n",
        "\n",
        "t0 = time.time()\n",
        "hyps = []\n",
        "for i in tqdm(range(0, len(sources), GEN_BATCH_SIZE), desc=\"Generating\"):\n",
        "    hyps.extend(generate_batch_chatml(sources[i:i+GEN_BATCH_SIZE]))\n",
        "t1 = time.time()\n",
        "assert len(hyps) == len(refs)\n",
        "print(f\"\\nGeneration: {len(hyps)} examples in {t1-t0:.1f}s\")\n",
        "\n",
        "# ----------------------- COMET (wmt22-da) -----------------------\n",
        "model_path  = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "comet_model = load_from_checkpoint(model_path)\n",
        "data = [{\"src\": s, \"mt\": h, \"ref\": r} for s, h, r in zip(sources, hyps, refs)]\n",
        "\n",
        "# Free VRAM\n",
        "try: model.to(\"cpu\")\n",
        "except: pass\n",
        "del enc, outputs\n",
        "import gc; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "t0 = time.time()\n",
        "res = comet_model.predict(\n",
        "    data,\n",
        "    batch_size=COMET_BATCH_SIZE,\n",
        "    gpus=(1 if torch.cuda.is_available() else 0),\n",
        ")\n",
        "t1 = time.time()\n",
        "\n",
        "#Normalize output\n",
        "if isinstance(res, tuple):\n",
        "    seg_scores, sys_score = res\n",
        "else:\n",
        "    seg_scores = res.get(\"segments_scores\") or res.get(\"segment_scores\") or res.get(\"scores\")\n",
        "    sys_score  = res.get(\"system_score\")   or res.get(\"score\") or res.get(\"mean\")\n",
        "\n",
        "def to_float_safe(x):\n",
        "    try: return float(x)\n",
        "    except: return float(\"nan\")\n",
        "\n",
        "sys_score_f = to_float_safe(sys_score)\n",
        "print(f\"COMET (wmt22-da) system: {sys_score_f:.4f} | {len(data)} segs in {t1-t0:.1f}s\")\n",
        "print(\"First 5 seg scores:\", (seg_scores[:5] if isinstance(seg_scores, list) else seg_scores))\n",
        "\n",
        "# ----------------------- Save outputs -----------------------\n",
        "import json, time, numpy as np, os\n",
        "RUN_STAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "DECODE_MODE = \"greedy\" if not DO_SAMPLE else \"sample\"\n",
        "TAG = f\"{SRC}-{TGT}_{SPLIT}_b{GEN_BATCH_SIZE}_t{MAX_NEW_TOKENS}_{DECODE_MODE}\"\n",
        "OUT_DIR = os.path.join(BASE_DIR, f\"{TAG}_{RUN_STAMP}\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"src\": sources,\n",
        "    \"hyp\": hyps,\n",
        "    \"ref\": refs,\n",
        "    \"comet\": np.asarray(seg_scores, dtype=np.float32)[:len(hyps)],\n",
        "})\n",
        "csv_path = os.path.join(OUT_DIR, \"segments.csv\")\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "summary = {\n",
        "    \"split\": SPLIT,\n",
        "    \"n_examples\": len(hyps),\n",
        "    \"system_score_comet_da\": float(sys_score_f),\n",
        "    \"decode\": {\n",
        "        \"mode\": DECODE_MODE,\n",
        "        \"max_new_tokens\": int(MAX_NEW_TOKENS),\n",
        "        \"gen_batch_size\": int(GEN_BATCH_SIZE),\n",
        "        \"do_sample\": bool(DO_SAMPLE),\n",
        "    },\n",
        "    \"model\": {\"hf_id\": globals().get(\"MODEL_ID\", None)},\n",
        "    \"paths\": {\"segments_csv\": csv_path},\n",
        "    \"timestamp\": RUN_STAMP,\n",
        "}\n",
        "with open(os.path.join(OUT_DIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"hypotheses.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for h in hyps: f.write(h.replace(\"\\n\", \" \").strip() + \"\\n\")\n",
        "\n",
        "print(\"Saved in:\", OUT_DIR)\n",
        "print(\"COMET:\", round(sys_score_f, 4))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyMHTI+AWJx2SbLp8Rgausf/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}