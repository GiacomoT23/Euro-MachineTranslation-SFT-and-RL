# ðŸŒ Euro Machine Translation (WIP)

![status](https://img.shields.io/badge/status-WIP-orange?style=flat-square)
![model](https://img.shields.io/badge/model-Qwen2.5_1.5B_base-blueviolet?style=flat-square)
![compute](https://img.shields.io/badge/compute-Colab-lightgrey?style=flat-square)

Reproducible MT experiments with a **compact LLM** under limited compute.  
Initial focus on **Germanâ†’English (WMT14)**, with plans to extend to other European language pairs.

**Phase 1:** instruction-style **SFT** with translation prompts.  
**Phase 2:** **GRPO** (RL) to compare against the SFT baseline under the same constraints.

---

## ðŸŽ¯ Goals

- **Model:** Qwen2.5 1.5B (base) â€” small enough for Colab.
- **Data:** WMT14 (DEâ†”EN) â€” standard, comparable, and well-known.
- **Method:** build a solid SFT baseline â†’ then add GRPO with automatic rewards.
- **Constraint:** keep it feasible on â€œconsumer/freeâ€ GPUs and reasonable wall time.

---

## âœ… Whatâ€™s implemented so far

### 1) Data prep & cleaning
- Downloaded **WMT14 DEâ€“EN** and ran **exploration** (length stats & distributions).
- **Language ID (fastText lid218e):** threshold sweep with **keep-rate** curves to pick a sensible cutoff. This is to ensure a good confidence in the src language being German and the target language being English.
- **Semantic alignment:** cosine similarity on samples (encoder: `intfloat/multilingual-e5-small`) to set a data-quality threshold.
- **Training-set filters** (dev/test kept **untouched** for fair evaluation):
  - **Length cap:** `len(src)+len(tgt) â‰¤ 203` (99 percentile of the distribution).
  - **Length ratio (tokens) between src and tgt:** interval configurable; explored 0.5â€“2, 0.33â€“3, 0.25â€“4.
  - **LID:** `P(src=deu_Latn) â‰¥ 0.99` and `P(tgt=eng_Latn) â‰¥ 0.99`.
  - **Cosine:** `cos(src,tgt) â‰¥ 0.88`.
- **Training sample:** built a **100k**-example train set and then filtered (remaining around 63 k); **validation/test are the original WMT14 splits** for comparability.

### 2) SFT pipeline
- Instruction-style **Source/Translation** prompt format.
- SFT with **TRL + Unsloth (QLoRA)** on Qwen2.5-1.5B.
- **Weights & Biases (W&B)** logging, **early stopping**, and **best checkpoint** exported to Drive.
- In-loop evaluation with **validation loss**.

> **Note:** exact training hyperparameters (LR/scheduler/batch, etc.) are **intentionally not fixed here** as theyâ€™re still under exploration.

---

## ðŸ§ª Experimental plan

### Phase 1 â€” SFT (ongoing)
- Goal: establish a solid DEâ†’EN baseline.
- Metrics: **COMET** for final reports.
- Checkpointing: best model saved.

### Phase 2 â€” RL with GRPO (next)
- Reward: sentence-level COMET or a hybrid (fluency/adequacy).
- Setup: same dev/test to compare fairly with the SFT baseline.

---

## ðŸ§° Stack

- **HF:** Transformers Â· TRL Â· Datasets Â· PEFT Â· bitsandbytes  
- **Modeling:** QLoRA (4-bit), Unsloth  
- **Embeddings:** `intfloat/multilingual-e5-small` (semantic similarity)  
- **LID:** fastText lid218e  
- **Eval:** sacreBLEU/chrF (subset in-loop), **COMET** for final evaluation  
- **Tracking:** Weights & Biases (W&B)  
- **Runtime:** Google Colab

---

## ðŸ“‚ Structure & reproducibility

- `de-en_preprocessing.ipynb` â€” **download, exploration, filtering, save to Arrow** (Drive)  
- `de-en_SFT.ipynb` â€” **load dataset, prompt formatting, SFT**, W&B, early stopping, **export best checkpoint**

---

> This repository is **work in progress**. Weâ€™ll keep updating it with results, notes, and emerging best practices.
