# ğŸš§ LLM Machine Translation â€” DEâ†”EN (WIP)

![status](https://img.shields.io/badge/status-WIP-orange?style=flat-square)
![model](https://img.shields.io/badge/model-Qwen_2.5_1.5B_base-blueviolet?style=flat-square)
![compute](https://img.shields.io/badge/compute-Colab_free-lightgrey?style=flat-square)

A lightweight, reproducible exploration of **machine translation** with a **small LLM** on a low resource setup.
The project is inspired by WMTâ€™s General MT spirit but deliberately scoped for tight compute.

**Phase 1:** supervised fine-tuning (**SFT**), initially on a single common language pair with a short, explicit translation prompt.  
**Phase 2:** **RL alignment** via **GRPO** to test whether we can push quality beyond SFT baselines under the same constraints.

> Work in progress â€” minimal first..

---

## ğŸ¯ Motivation & Scope

- **Why small LLM (~1â€“2B)**: fits free Colab VRAM, enables rapid iteration.
- **Why starting with DEâ†”EN (WMT14)**: classic pair with well-understood baselines; easy to sanity-check progress.
- **Why prompt + SFT**: explicitly conditions the base model on translation instructions without heavy scaffolding.
- **Why GRPO next**: study whether lightweight RL can improve adequacy/fluency/faithfulness beyond SFT with limited resources.

---

## ğŸ“ Prompting Strategy (SFT)

Each example is formatted with a brief instruction, followed by source/target fields.

---

## ğŸ§ª Plan

### Phase 1 â€” Supervised Fine-Tuning (SFT)
- **Model**: `Qwen 2.5 1.5B (base)`
- **Data**: WMT14 DEâ†”EN (train â†’ train/val; standard dev/test later)
- **Training**: LoRA/QLoRA (4-bit), mixed precision, gradient accumulation
- **Evaluation**: report scores with **consolidated MT metrics** (e.g., BLEU, chrF, COMET) + a short qualitative error analysis

### Phase 2 â€” RL with GRPO (Planned)
- **Goal**: align behavior
- **Reward**: to be finalized (e.g., sentence-level BLEU/chrF or hybrid signals)
- **Comparison**: SFT vs. SFT+GRPO on the same test split and prompts

---

## ğŸ§° Tech Stack (tentative)

- **Core**: PyTorch Â· Transformers Â· PEFT (LoRA/QLoRA) Â· bitsandbytes (quantization)
- **Runtime**: Colab (free tier), mixed precision + gradient accumulation
- **Configs & Logs**: simple YAML/JSON for hyper-params; lightweight CSV/stdout logs

---

## ğŸ“ Training Recipe (SFT â€” initial target)

- **Quantization**: 4-bit QLoRA (if VRAM tight), otherwise LoRA
- **Batching**: small micro-batches + **gradient accumulation**
- **Scheduler**: cosine/warmup (simple, stable), early-stop on val
- **Regularization**: label smoothing optional (to compare)
- **Tokenization**: modelâ€™s native tokenizer; normalize punctuation/casing consistently

---

## ğŸ“ Evaluation Plan

- **Metrics (generic)**: consolidated machine translation metrics  
  *(examples: BLEU, chrF, COMET â€” final choice TBD)*
- **Qualitative**: inspect typical errors (idioms, named entities, length/coverage)
- **Efficiency**: tokens/s, VRAM footprint, runtime per epoch (recorded in logs)
- **Reporting**: short **result card** per run (data version, config, metrics, sample outputs)

---

## ğŸ—ºï¸ Roadmap

- [ ] Minimal repo skeleton + placeholder scripts
- [ ] Data prep for WMT14 DEâ†”EN + prompt formatting
- [ ] SFT pilot (DEâ†’EN), then add ENâ†’DE
- [ ] Baseline report with consolidated metrics + sample translations
- [ ] Implement **GRPO** training loop (reward TBD) and compare vs SFT
- [ ] Cleanup + â€œHow to reproduceâ€ section and final configs

